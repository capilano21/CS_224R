{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0g8dQY4gM8"
      },
      "outputs": [],
      "source": [
        "### Install Dependencies ###\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3W1L0SAMM-"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "WIN = 1.0\n",
        "LOSS = -1.0\n",
        "DRAW = 1.0\n",
        "BLACK = True\n",
        "EPS = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QKluSh4DFd"
      },
      "outputs": [],
      "source": [
        "class Board():\n",
        "  def __init__(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "    self.eps = EPS\n",
        "  def reset(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "  def play_move(self,pos):\n",
        "    if(self.state[pos]!=0.0):\n",
        "        raise Exception(\"You made an illegal move\")\n",
        "\n",
        "    self.state[pos]=self.marker\n",
        "    self.marker = self.marker *-1.0\n",
        "  def get_status(self):\n",
        "\n",
        "    winning_combinations = [[0,1,2],[3,4,5],[6,7,8],\n",
        "                            [0,3,6],[1,4,7],[2,5,8],\n",
        "                            [0,4,8],[2,4,6]]\n",
        "\n",
        "    for combination in winning_combinations:\n",
        "           if(self.state[combination[0]]==self.state[combination[1]]==self.state[combination[2]]!=0.0):\n",
        "               self.game_status = torch.tensor(1.0)\n",
        "               self.result = self.state[combination[0]]\n",
        "               if(BLACK):\n",
        "                 self.result = self.result*-1.0\n",
        "    if(torch.count_nonzero(self.state)==9.0):\n",
        "        self.game_status = torch.tensor(1.0)\n",
        "        self.result = torch.tensor(DRAW)\n",
        "    return self.game_status,self.result\n",
        "  def get_mask(self):\n",
        "     mask = self.state==0.0\n",
        "     return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TicTacToeMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TicTacToePolicy:\n",
        "    def __init__(self,default=False):\n",
        "        self.model = TicTacToeMLP(9, 128, 9)  # Input: 9 (3x3 board), Hidden: 128, Output: 9 (actions)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=5e-2)\n",
        "        self.eval = False\n",
        "        self.default = default\n",
        "\n",
        "    def get_action_probabilities(self, board):\n",
        "        inp = board.state\n",
        "\n",
        "        logit = self.model(inp.clone())\n",
        "\n",
        "\n",
        "        # Mask out illegal actions\n",
        "        mask =   board.get_mask() # Mask: 1 for legal actions, 0 for illegal actions\n",
        "\n",
        "        mask = mask.float()\n",
        "\n",
        "        logits = logit - 1e10 * (1 - mask)  # Apply large negative values to illegal actions\n",
        "        probabilities = F.softmax(logits, dim=0)\n",
        "        if(eval):\n",
        "          move = torch.argmax(probabilities)\n",
        "        m =   torch.distributions.categorical.Categorical(probs=probabilities)\n",
        "        move = m.sample()\n",
        "\n",
        "        step = torch.count_nonzero(mask)\n",
        "        if(self.default):\n",
        "          eps_temp = 0.1\n",
        "        else:\n",
        "          eps_temp = board.eps*(1-step/12)\n",
        "\n",
        "        if(random.random()<eps_temp):\n",
        "          valid_moves = np.where(mask==1.0)[0]\n",
        "          move = np.random.choice(valid_moves)\n",
        "          move = torch.tensor(move)\n",
        "\n",
        "        log_prob = m.log_prob(move)\n",
        "        return log_prob,move.item()"
      ],
      "metadata": {
        "id": "bCCbGXyRxl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oNIoTXAVEc8"
      },
      "outputs": [],
      "source": [
        "class RandomPlayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, board):\n",
        "        # Get a list of available actions\n",
        "        pos = np.where(board.get_mask()==1)[0]\n",
        "        move = np.random.choice(pos)\n",
        "\n",
        "\n",
        "        return move\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO-0DIJDFH4M"
      },
      "outputs": [],
      "source": [
        "class V(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(V, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "v_net = V(9,128,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB3fhJfcUZsq"
      },
      "outputs": [],
      "source": [
        "board = Board()\n",
        "player_1 = TicTacToePolicy(default=True)\n",
        "player_1.model.load_state_dict(torch.load(\"/content/drive/MyDrive/policy.pt\"))\n",
        "\n",
        "player_2 = RandomPlayer()\n",
        "\n",
        "player_3 = TicTacToePolicy()\n",
        "\n",
        "\n",
        "def games(player=\"random\"):\n",
        "  states = []\n",
        "  log_prob = []\n",
        "\n",
        "  board.reset()\n",
        "  while True:\n",
        "\n",
        "    prob,move = player_1.get_action_probabilities(board)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "    states.append(board.state)\n",
        "\n",
        "    if(player==\"random\"):\n",
        "       move = player_2.get_action(board)\n",
        "    else:\n",
        "       prob,move = player_3.get_action_probabilities(board)\n",
        "    log_prob.append(prob)\n",
        "\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "  return states,reward,log_prob\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG78lMCAKiVG",
        "outputId": "65777201-6258-48ab-dccd-ddb9854e5010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "v_optimizer = torch.optim.Adam(v_net.parameters(),lr =1e-2)\n"
      ],
      "metadata": {
        "id": "tUDrd_2EkXZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r runs"
      ],
      "metadata": {
        "id": "D45xPe0fKxhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Define Training Loop ##\n",
        "num_games = 100000\n",
        "board.eps=0.6\n",
        "player_3.optimizer.zero_grad()\n",
        "for playouts in range(num_games):\n",
        "  states,reward,log_prob= games(\"neural\")\n",
        "  reward = -(reward-(0.8))**2\n",
        "  writer.add_scalar('Episode Reward', reward, playouts)\n",
        "  states_tensor = torch.stack(states)\n",
        "  states_tensor.shape\n",
        "  log_prob_tensor = torch.stack(log_prob)\n",
        "  log_prob_tensor.shape\n",
        "  reward_tensor = torch.ones(log_prob_tensor.shape)*reward\n",
        "  b = v_net(states_tensor)\n",
        "  loss =criterion(b.squeeze(),reward_tensor)\n",
        "  writer.add_scalar('Value_Loss', loss.item(), playouts)\n",
        "  v_optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  v_optimizer.step()\n",
        "  b = v_net(states_tensor)\n",
        "  rewards_baseline = reward_tensor -b.detach().squeeze()\n",
        "  grad_tensor = (-(log_prob_tensor)*(rewards_baseline)).sum()\n",
        "  grad_tensor.backward()\n",
        "  board.eps = board.eps-board.eps/(num_games-20000)\n",
        "\n",
        "  if((playouts+1)%512==0):\n",
        "     player_3.optimizer.step()\n",
        "     player_3.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if(playouts%10000==0):\n",
        "     player_3.eval=False\n",
        "     loss = 0\n",
        "     draw = 0\n",
        "     win = 0\n",
        "     for i in range(1000):\n",
        "      _,reward,_ = games(\"neural\")\n",
        "      if(reward==1.0):\n",
        "        draw+=1           ## Note: We are rewarding draws and wins as 1 to calculate ELO, only score matters\n",
        "      elif(reward==-1.0):\n",
        "        loss+=1\n",
        "      else:\n",
        "        win+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     writer.add_scalar(\"Win_percentage\", win*100,playouts)\n",
        "     writer.add_scalar(\"Loss_percentage\",loss*100,playouts)\n",
        "     writer.add_scalar(\"Draw_percentage\",draw*100,playouts)\n",
        "     print(\"Evaluation after\",playouts,\"games\")\n",
        "     print(\"Win\",win/1000)\n",
        "     print(\"Loss\",loss/1000)\n",
        "     print(\"Draw\",draw/1000)\n",
        "     player_1.eval=False\n",
        "### Final testing against minimax,minimx_random and random ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(player_3.model.state_dict(),\"/content/drive/MyDrive/policy_exact.pt\")\n",
        "#torch.save(v_net.state_dict(),\"/content/drive/MyDrive/value.pt\")\n",
        "\n",
        "writer.add_scalar(\"Win_percentage_final\", win*100)\n",
        "writer.add_scalar(\"Loss_percentage_final\",loss*100)\n",
        "writer.add_scalar(\"Draw_percentage_final\",draw*100)\n",
        "print(\"Evaluation after 1000 games\")\n",
        "print(\"Win\",win/1000)\n",
        "print(\"Loss\",loss/1000)\n",
        "print(\"Draw\",draw/1000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc2uHaTpief_",
        "outputId": "e9d01beb-7b26-4a11-84f3-27c16f2a8fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after 0 games\n",
            "Win 0.0\n",
            "Loss 0.753\n",
            "Draw 0.247\n",
            "Evaluation after 10000 games\n",
            "Win 0.0\n",
            "Loss 0.742\n",
            "Draw 0.258\n",
            "Evaluation after 20000 games\n",
            "Win 0.0\n",
            "Loss 0.761\n",
            "Draw 0.239\n",
            "Evaluation after 30000 games\n",
            "Win 0.0\n",
            "Loss 0.719\n",
            "Draw 0.281\n",
            "Evaluation after 40000 games\n",
            "Win 0.0\n",
            "Loss 0.712\n",
            "Draw 0.288\n",
            "Evaluation after 50000 games\n",
            "Win 0.0\n",
            "Loss 0.71\n",
            "Draw 0.29\n",
            "Evaluation after 60000 games\n",
            "Win 0.0\n",
            "Loss 0.724\n",
            "Draw 0.276\n",
            "Evaluation after 70000 games\n",
            "Win 0.0\n",
            "Loss 0.727\n",
            "Draw 0.273\n",
            "Evaluation after 80000 games\n",
            "Win 0.0\n",
            "Loss 0.698\n",
            "Draw 0.302\n",
            "Evaluation after 90000 games\n",
            "Win 0.0\n",
            "Loss 0.693\n",
            "Draw 0.307\n",
            "Evaluation after 1000 games\n",
            "Win 0.0\n",
            "Loss tensor(0.0092, grad_fn=<DivBackward0>)\n",
            "Draw 0.307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(player_1.model.state_dict(),\"/content/weights.pt\")\n",
        "torch.save(v_net.state_dict(),\"/content/v_net.pt\")"
      ],
      "metadata": {
        "id": "GjXoVJNb2Ln7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r runs_exact_strength /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "ZThZwnrCTcAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}