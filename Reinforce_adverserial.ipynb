{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0g8dQY4gM8"
      },
      "outputs": [],
      "source": [
        "### Install Dependencies ###\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3W1L0SAMM-"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "WIN = 1.0\n",
        "LOSS = -1.0\n",
        "DRAW = 0.5\n",
        "EPS = 0.7\n",
        "DEVICE = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QKluSh4DFd"
      },
      "outputs": [],
      "source": [
        "class Board():\n",
        "  def __init__(self,mark=1.0):\n",
        "    self.mark = mark\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "    self.eps = EPS\n",
        "  def reset(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "  def play_move(self,pos):\n",
        "    if(self.state[pos]!=0.0):\n",
        "        raise Exception(\"You made an illegal move\")\n",
        "\n",
        "    self.state[pos]=self.marker\n",
        "    self.marker = self.marker *-1.0\n",
        "  def get_status(self):\n",
        "\n",
        "    winning_combinations = [[0,1,2],[3,4,5],[6,7,8],\n",
        "                            [0,3,6],[1,4,7],[2,5,8],\n",
        "                            [0,4,8],[2,4,6]]\n",
        "\n",
        "    for combination in winning_combinations:\n",
        "           if(self.state[combination[0]]==self.state[combination[1]]==self.state[combination[2]]!=0.0):\n",
        "               self.game_status = torch.tensor(1.0)\n",
        "               self.result = self.state[combination[0]]*self.mark\n",
        "    if(torch.count_nonzero(self.state)==9.0):\n",
        "        self.game_status = torch.tensor(1.0)\n",
        "        self.result = torch.tensor(DRAW)\n",
        "    return self.game_status,self.result\n",
        "  def get_mask(self):\n",
        "     mask = self.state==0.0\n",
        "     return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TicTacToeMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TicTacToePolicy:\n",
        "    def __init__(self,default=False):\n",
        "        self.default = default\n",
        "        self.model = TicTacToeMLP(9, 128, 9).to(torch.device(DEVICE))  # Input: 9 (3x3 board), Hidden: 128, Output: 9 (actions)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-2)\n",
        "        self.eval = False\n",
        "\n",
        "    def get_action_probabilities(self, board):\n",
        "        inp = board.state\n",
        "\n",
        "        logit = self.model(inp.clone().to(torch.device(DEVICE)))\n",
        "\n",
        "\n",
        "        # Mask out illegal actions\n",
        "        mask =   board.get_mask() # Mask: 1 for legal actions, 0 for illegal actions\n",
        "\n",
        "        mask = mask.float().to(torch.device(DEVICE))\n",
        "\n",
        "        logits = logit - 1e9 * (1 - mask)  # Apply large negative values to illegal actions\n",
        "        probabilities = F.softmax(logits, dim=0)\n",
        "        if(eval):\n",
        "          move = torch.argmax(probabilities)\n",
        "        m =   torch.distributions.categorical.Categorical(probs=probabilities)\n",
        "        move = m.sample()\n",
        "\n",
        "        step = torch.count_nonzero(mask)\n",
        "        if(self.default):\n",
        "          eps_temp = 0.1\n",
        "        else:\n",
        "          eps_temp = board.eps*(1-step/12)\n",
        "\n",
        "        if(random.random()<eps_temp):\n",
        "          valid_moves = torch.where(mask==1.0)[0]\n",
        "          move = np.random.choice(valid_moves.cpu())\n",
        "          move = torch.tensor(move).to(torch.device(DEVICE))\n",
        "\n",
        "\n",
        "        log_prob = m.log_prob(move)\n",
        "        return log_prob,move.item()"
      ],
      "metadata": {
        "id": "bCCbGXyRxl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oNIoTXAVEc8"
      },
      "outputs": [],
      "source": [
        "class RandomPlayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, board):\n",
        "        # Get a list of available actions\n",
        "        pos = np.where(board.get_mask()==1)[0]\n",
        "        move = np.random.choice(pos)\n",
        "\n",
        "\n",
        "        return move\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO-0DIJDFH4M"
      },
      "outputs": [],
      "source": [
        "class V(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(V, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "v_net_1 = V(9,128,1).to(torch.device(DEVICE))\n",
        "v_net_3 = V(9,128,1).to(torch.device(DEVICE))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB3fhJfcUZsq"
      },
      "outputs": [],
      "source": [
        "board = Board()\n",
        "player_1 = TicTacToePolicy()\n",
        "player_1.model.load_state_dict(torch.load(\"/content/drive/MyDrive/attack_o_1.pt\"))\n",
        "\n",
        "player_2 = TicTacToePolicy(default=True)\n",
        "player_2.model.load_state_dict(torch.load(\"/content/drive/MyDrive/o_strategy450000.pt\"))\n",
        "\n",
        "player_3 = TicTacToePolicy()\n",
        "#player_3.model.load_state_dict(torch.load(\"/content/drive/MyDrive/o_strategy250000.pt\"))\n",
        "\n",
        "\n",
        "\n",
        "def attack(adv=False):\n",
        "  states = []\n",
        "  log_prob = []\n",
        "\n",
        "  board.reset()\n",
        "  while True:\n",
        "    states.append(board.state)\n",
        "    prob,move = player_1.get_action_probabilities(board)\n",
        "    log_prob.append(prob)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "    if(adv==False):\n",
        "        prob,move = player_2.get_action_probabilities(board)\n",
        "    else:\n",
        "        prob,move = player_3.get_action_probabilities(board)\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "\n",
        "\n",
        "    if(status==1.0):\n",
        "       break\n",
        "  return states,reward,log_prob\n",
        "\n",
        "def defense(mark=-1.0):\n",
        "  states = []\n",
        "  log_prob = []\n",
        "\n",
        "  board.reset()\n",
        "  if(mark==-1.0):\n",
        "    board.mark=-1.0\n",
        "\n",
        "  while True:\n",
        "\n",
        "    _,move = player_1.get_action_probabilities(board)\n",
        "\n",
        "\n",
        "    board.play_move(move)\n",
        "    board.mark=-1.0\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "    states.append(board.state)\n",
        "\n",
        "    prob,move = player_3.get_action_probabilities(board)\n",
        "    board.play_move(move)\n",
        "    board.mark=-1.0\n",
        "    status,reward =board.get_status()\n",
        "    log_prob.append(prob)\n",
        "    if(status==1.0):\n",
        "       break\n",
        "  board.mark=1.0\n",
        "  return states,reward,log_prob\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rG78lMCAKiVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4373cb88-27e5-4074-b805-db3f9ad0c3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "v_optimizer = torch.optim.Adam(v_net_1.parameters(),lr =1e-2)\n",
        "a_optimizer = torch.optim.Adam(v_net_3.parameters(),lr =1e-2)\n"
      ],
      "metadata": {
        "id": "tUDrd_2EkXZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r runs"
      ],
      "metadata": {
        "id": "D45xPe0fKxhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Define Training Loop ##\n",
        "num_games = 100000\n",
        "board.eps=0.5\n",
        "player_1.optimizer.zero_grad()\n",
        "player_3.optimizer.zero_grad()\n",
        "for playouts in range(num_games):\n",
        "  states,reward,log_prob= attack()\n",
        "  #writer.add_scalar('Episode Reward', reward, playouts)\n",
        "  states_tensor = torch.stack(states)\n",
        "  log_prob_tensor = torch.stack(log_prob)\n",
        "  reward_tensor = torch.ones(log_prob_tensor.shape)*reward\n",
        "  reward_tensor = reward_tensor.to(torch.device(DEVICE))\n",
        "  b = v_net_1(states_tensor.to(torch.device(DEVICE)))\n",
        "  loss_1 =criterion(b.squeeze(),reward_tensor)\n",
        "  #writer.add_scalar('Value_Loss', loss_1.item(), playouts)\n",
        "  v_optimizer.zero_grad()\n",
        "  loss_1.backward()\n",
        "  v_optimizer.step()\n",
        "  b = v_net_1(states_tensor.to(torch.device(DEVICE)))\n",
        "  rewards_baseline = reward_tensor -b.detach().squeeze()\n",
        "  grad_tensor = (-(log_prob_tensor)*(rewards_baseline)).sum()\n",
        "  grad_tensor.backward()\n",
        "\n",
        "  states,reward,log_prob= defense(mark=-1.0)\n",
        "  states_tensor = torch.stack(states)\n",
        "  log_prob_tensor = torch.stack(log_prob)\n",
        "  reward_tensor = torch.ones(log_prob_tensor.shape)*reward\n",
        "  reward_tensor = reward_tensor.to(torch.device(DEVICE))\n",
        "  b = v_net_3(states_tensor.to(torch.device(DEVICE)))\n",
        "  loss_2 =criterion(b.squeeze(),reward_tensor)\n",
        "  a_optimizer.zero_grad()\n",
        "  loss_2.backward()\n",
        "  a_optimizer.step()\n",
        "  b = v_net_3(states_tensor.to(torch.device(DEVICE)))\n",
        "  rewards_baseline = reward_tensor -b.detach().squeeze()\n",
        "  grad_tensor_1 = ((-log_prob_tensor)*(rewards_baseline)).sum()\n",
        "  grad_tensor_1.backward()\n",
        "\n",
        "  if(playouts%1000==0):\n",
        "    print(playouts)\n",
        "\n",
        "  states,reward,log_prob= attack(adv=True)\n",
        "  states_tensor = torch.stack(states)\n",
        "  log_prob_tensor = torch.stack(log_prob)\n",
        "  reward_tensor =  torch.ones(log_prob_tensor.shape)*reward\n",
        "  reward_tensor = reward_tensor.to(torch.device(DEVICE))\n",
        "  b = v_net_1(states_tensor.to(torch.device(DEVICE)))\n",
        "  loss_3 =criterion(b.squeeze(),reward_tensor)\n",
        "  loss_3 = loss_3*2.5\n",
        "  v_optimizer.zero_grad()\n",
        "  loss_3.backward()\n",
        "  v_optimizer.step()\n",
        "  b = v_net_1(states_tensor.to(torch.device(DEVICE)))\n",
        "  rewards_baseline = reward_tensor -b.detach().squeeze()\n",
        "  grad_tensor = (-(log_prob_tensor)*(rewards_baseline)).sum()\n",
        "  grad_tensor.backward()\n",
        "\n",
        "  board.eps = board.eps-board.eps/(num_games-30000)\n",
        "\n",
        "  if((playouts+1)%64==0):\n",
        "    player_3.optimizer.step()\n",
        "    player_3.optimizer.zero_grad()\n",
        "\n",
        "  if((playouts+1)%512==0):\n",
        "     player_1.optimizer.step()\n",
        "     player_1.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if(playouts%10000==0):\n",
        "     player_1.eval=True\n",
        "     loss = 0\n",
        "     draw = 0\n",
        "     win = 0\n",
        "     for i in range(1000):\n",
        "      _,reward,_ = attack()\n",
        "      if(reward==1.0):\n",
        "        win+=1\n",
        "      elif(reward==-1.0):\n",
        "        loss+=1\n",
        "      else:\n",
        "        draw+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     writer.add_scalar(\"Win_percentage_mined\", win*100,playouts)\n",
        "     writer.add_scalar(\"Loss_percentage_mined\",loss*100,playouts)\n",
        "     writer.add_scalar(\"Draw_percentage_mined\",draw*100,playouts)\n",
        "     print(\"Evaluation after\",playouts,\"games\")\n",
        "     print(\"Win\",win/1000)\n",
        "     print(\"Loss\",loss/1000)\n",
        "     print(\"Draw\",draw/1000)\n",
        "     player_1.eval=False\n",
        "  if(playouts%10000==0):\n",
        "     player_1.eval=True\n",
        "     player_3.eval=True\n",
        "     loss = 0\n",
        "     draw = 0\n",
        "     win = 0\n",
        "     for i in range(1000):\n",
        "      _,reward,_ = defense()\n",
        "      if(reward==1.0):\n",
        "        win+=1\n",
        "      elif(reward==-1.0):\n",
        "        loss+=1\n",
        "      else:\n",
        "        draw+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     writer.add_scalar(\"Win_percentage_adverserial\", win*100,playouts)\n",
        "     writer.add_scalar(\"Loss_percentage_adverserial\",loss*100,playouts)\n",
        "     writer.add_scalar(\"Draw_percentage_adverserial\",draw*100,playouts)\n",
        "     print(\"Evaluation after\",playouts,\"games\")\n",
        "     print(\"Win\",win/1000)\n",
        "     print(\"Loss\",loss/1000)\n",
        "     print(\"Draw\",draw/1000)\n",
        "     player_1.eval=False\n",
        "     player_3.eval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc2uHaTpief_",
        "outputId": "81964132-8432-417f-9aa8-1b73929ef145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Evaluation after 0 games\n",
            "Win 0.699\n",
            "Loss 0.168\n",
            "Draw 0.133\n",
            "Evaluation after 0 games\n",
            "Win 0.192\n",
            "Loss 0.701\n",
            "Draw 0.107\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "Evaluation after 10000 games\n",
            "Win 0.741\n",
            "Loss 0.16\n",
            "Draw 0.099\n",
            "Evaluation after 10000 games\n",
            "Win 0.115\n",
            "Loss 0.83\n",
            "Draw 0.055\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "Evaluation after 20000 games\n",
            "Win 0.749\n",
            "Loss 0.17\n",
            "Draw 0.081\n",
            "Evaluation after 20000 games\n",
            "Win 0.079\n",
            "Loss 0.877\n",
            "Draw 0.044\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "Evaluation after 30000 games\n",
            "Win 0.777\n",
            "Loss 0.152\n",
            "Draw 0.071\n",
            "Evaluation after 30000 games\n",
            "Win 0.059\n",
            "Loss 0.899\n",
            "Draw 0.042\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "Evaluation after 40000 games\n",
            "Win 0.78\n",
            "Loss 0.154\n",
            "Draw 0.066\n",
            "Evaluation after 40000 games\n",
            "Win 0.049\n",
            "Loss 0.927\n",
            "Draw 0.024\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "Evaluation after 50000 games\n",
            "Win 0.817\n",
            "Loss 0.127\n",
            "Draw 0.056\n",
            "Evaluation after 50000 games\n",
            "Win 0.028\n",
            "Loss 0.939\n",
            "Draw 0.033\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "Evaluation after 60000 games\n",
            "Win 0.807\n",
            "Loss 0.144\n",
            "Draw 0.049\n",
            "Evaluation after 60000 games\n",
            "Win 0.02\n",
            "Loss 0.97\n",
            "Draw 0.01\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "Evaluation after 70000 games\n",
            "Win 0.81\n",
            "Loss 0.143\n",
            "Draw 0.047\n",
            "Evaluation after 70000 games\n",
            "Win 0.018\n",
            "Loss 0.967\n",
            "Draw 0.015\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "Evaluation after 80000 games\n",
            "Win 0.837\n",
            "Loss 0.121\n",
            "Draw 0.042\n",
            "Evaluation after 80000 games\n",
            "Win 0.017\n",
            "Loss 0.973\n",
            "Draw 0.01\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "Evaluation after 90000 games\n",
            "Win 0.825\n",
            "Loss 0.12\n",
            "Draw 0.055\n",
            "Evaluation after 90000 games\n",
            "Win 0.01\n",
            "Loss 0.986\n",
            "Draw 0.004\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/attack_o_60.pt\")\n",
        "#torch.save(player_3.model.state_dict(),\"/content/adv_o-1.pt\")"
      ],
      "metadata": {
        "id": "GjXoVJNb2Ln7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r runs_60_o /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "lRsY0vDFR3Hz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}