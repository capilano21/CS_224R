{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0g8dQY4gM8"
      },
      "outputs": [],
      "source": [
        "### Install Dependencies ###\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3W1L0SAMM-"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "WIN = 1.0\n",
        "LOSS = -1.0\n",
        "DRAW = 0.5\n",
        "EPS = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QKluSh4DFd"
      },
      "outputs": [],
      "source": [
        "class Board():\n",
        "  def __init__(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "    self.eps = EPS\n",
        "  def reset(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "  def play_move(self,pos):\n",
        "    if(self.state[pos]!=0.0):\n",
        "        raise Exception(\"You made an illegal move\")\n",
        "\n",
        "    self.state[pos]=self.marker\n",
        "    self.marker = self.marker *-1.0\n",
        "  def get_status(self):\n",
        "\n",
        "    winning_combinations = [[0,1,2],[3,4,5],[6,7,8],\n",
        "                            [0,3,6],[1,4,7],[2,5,8],\n",
        "                            [0,4,8],[2,4,6]]\n",
        "\n",
        "    for combination in winning_combinations:\n",
        "           if(self.state[combination[0]]==self.state[combination[1]]==self.state[combination[2]]!=0.0):\n",
        "               self.game_status = torch.tensor(1.0)\n",
        "               self.result = self.state[combination[0]]\n",
        "    if(torch.count_nonzero(self.state)==9.0):\n",
        "        self.game_status = torch.tensor(1.0)\n",
        "        self.result = torch.tensor(0.5)\n",
        "    return self.game_status,self.result\n",
        "  def get_mask(self):\n",
        "     mask = self.state==0.0\n",
        "     return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TicTacToeMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TicTacToePolicy:\n",
        "    def __init__(self):\n",
        "        self.model = TicTacToeMLP(9, 128, 9)  # Input: 9 (3x3 board), Hidden: 128, Output: 9 (actions)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-2)\n",
        "        self.eval = False\n",
        "\n",
        "    def get_action_probabilities(self, board):\n",
        "        inp = board.state\n",
        "\n",
        "        logit = self.model(inp.clone())\n",
        "\n",
        "\n",
        "        # Mask out illegal actions\n",
        "        mask =   board.get_mask() # Mask: 1 for legal actions, 0 for illegal actions\n",
        "\n",
        "        mask = mask.float()\n",
        "\n",
        "        logits = logit - 1e9 * (1 - mask)  # Apply large negative values to illegal actions\n",
        "        probabilities = F.softmax(logits, dim=0)\n",
        "        if(eval):\n",
        "          move = torch.argmax(probabilities)\n",
        "        m =   torch.distributions.categorical.Categorical(probs=probabilities)\n",
        "        move = m.sample()\n",
        "\n",
        "        step = torch.count_nonzero(mask)\n",
        "        eps_temp = board.eps*(1-step/12)\n",
        "\n",
        "        if(random.random()<eps_temp):\n",
        "          valid_moves = np.where(mask==1.0)[0]\n",
        "          move = np.random.choice(valid_moves)\n",
        "          move = torch.tensor(move)\n",
        "\n",
        "        log_prob = m.log_prob(move)\n",
        "        return log_prob,move.item()"
      ],
      "metadata": {
        "id": "bCCbGXyRxl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oNIoTXAVEc8"
      },
      "outputs": [],
      "source": [
        "class RandomPlayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, board):\n",
        "        # Get a list of available actions\n",
        "        pos = np.where(board.get_mask()==1)[0]\n",
        "        move = np.random.choice(pos)\n",
        "\n",
        "\n",
        "        return move\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO-0DIJDFH4M"
      },
      "outputs": [],
      "source": [
        "class V(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(V, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "v_net = V(9,128,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB3fhJfcUZsq"
      },
      "outputs": [],
      "source": [
        "board = Board()\n",
        "player_1 = TicTacToePolicy()\n",
        "\n",
        "player_2 = RandomPlayer()\n",
        "\n",
        "\n",
        "def games(player=\"random\"):\n",
        "  states = []\n",
        "  log_prob = []\n",
        "\n",
        "  board.reset()\n",
        "  while True:\n",
        "    states.append(board.state)\n",
        "    prob,move = player_1.get_action_probabilities(board)\n",
        "    log_prob.append(prob)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "    if(player==\"random\"):\n",
        "       move = player_2.get_action(board)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "  return states,reward,log_prob\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG78lMCAKiVG",
        "outputId": "da4e945a-958c-44d8-8f20-7d291b52874b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "v_optimizer = torch.optim.Adam(v_net.parameters(),lr =1e-2)\n"
      ],
      "metadata": {
        "id": "tUDrd_2EkXZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r runs"
      ],
      "metadata": {
        "id": "D45xPe0fKxhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Define Training Loop ##\n",
        "num_games = 500000\n",
        "board.eps=0.6\n",
        "player_1.optimizer.zero_grad()\n",
        "for playouts in range(num_games):\n",
        "  states,reward,log_prob= games()\n",
        "  writer.add_scalar('Episode Reward', reward, playouts)\n",
        "  states_tensor = torch.stack(states)\n",
        "  states_tensor.shape\n",
        "  log_prob_tensor = torch.stack(log_prob)\n",
        "  log_prob_tensor.shape\n",
        "  reward_tensor = torch.ones(log_prob_tensor.shape)*reward\n",
        "  b = v_net(states_tensor)\n",
        "  loss =criterion(b.squeeze(),reward_tensor)\n",
        "  writer.add_scalar('Value_Loss', loss.item(), playouts)\n",
        "  v_optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  v_optimizer.step()\n",
        "  b = v_net(states_tensor)\n",
        "  rewards_baseline = reward_tensor -b.detach().squeeze()\n",
        "  grad_tensor = (-(log_prob_tensor)*(rewards_baseline)).sum()\n",
        "  grad_tensor.backward()\n",
        "  board.eps = board.eps-board.eps/(num_games-30000)\n",
        "  #board.eps = 0.0\n",
        "  #if(playouts>num_games-50000):\n",
        "    #board.eps=0\n",
        "\n",
        "  if((playouts+1)%512==0):\n",
        "     player_1.optimizer.step()\n",
        "     player_1.optimizer.zero_grad()\n",
        "\n",
        "  if(playouts==5000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "  if(playouts==15000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "  if(playouts==25000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "  if(playouts==60000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "  if(playouts==250000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "  if(playouts==450000):\n",
        "    torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy\"+str(playouts)+\".pt\")\n",
        "\n",
        "\n",
        "  if(playouts%10000==0):\n",
        "     player_1.eval=True\n",
        "     loss = 0\n",
        "     draw = 0\n",
        "     win = 0\n",
        "     for i in range(1000):\n",
        "      _,reward,_ = games()\n",
        "      if(reward==1.0):\n",
        "        win+=1\n",
        "      elif(reward==-1.0):\n",
        "        loss+=1\n",
        "      else:\n",
        "        draw+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     writer.add_scalar(\"Win_percentage_random\", win*100,playouts)\n",
        "     writer.add_scalar(\"Loss_percentage_random\",loss*100,playouts)\n",
        "     writer.add_scalar(\"Draw_percentage_random\",draw*100,playouts)\n",
        "     print(\"Evaluation after\",playouts,\"games\")\n",
        "     print(\"Win\",win/1000)\n",
        "     print(\"Loss\",loss/1000)\n",
        "     print(\"Draw\",draw/1000)\n",
        "     player_1.eval=False\n",
        "### Final testing against minimax,minimx_random and random ###\n",
        "player_1.eval=True\n",
        "loss = 0\n",
        "draw = 0\n",
        "win = 0\n",
        "for i in range(1000):\n",
        "    _,reward,_ = games(player=\"random\")\n",
        "    if(reward==1.0):\n",
        "      win+=1\n",
        "    elif(reward==-1.0):\n",
        "      loss+=1\n",
        "    else:\n",
        "      draw+=1\n",
        "\n",
        "\n",
        "\n",
        "torch.save(player_1.model.state_dict(),\"/content/drive/MyDrive/policy.pt\")\n",
        "torch.save(v_net.state_dict(),\"/content/drive/MyDrive/value.pt\")\n",
        "\n",
        "writer.add_scalar(\"Win_percentage_final\", win*100)\n",
        "writer.add_scalar(\"Loss_percentage_final\",loss*100)\n",
        "writer.add_scalar(\"Draw_percentage_final\",draw*100)\n",
        "print(\"Evaluation after 1000 games\")\n",
        "print(\"Win\",win/1000)\n",
        "print(\"Loss\",loss/1000)\n",
        "print(\"Draw\",draw/1000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc2uHaTpief_",
        "outputId": "c6b4a34e-5e60-43d8-bf1d-78cc6570b2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after 0 games\n",
            "Win 0.362\n",
            "Loss 0.299\n",
            "Draw 0.339\n",
            "Evaluation after 10000 games\n",
            "Win 0.722\n",
            "Loss 0.142\n",
            "Draw 0.136\n",
            "Evaluation after 20000 games\n",
            "Win 0.762\n",
            "Loss 0.135\n",
            "Draw 0.103\n",
            "Evaluation after 30000 games\n",
            "Win 0.754\n",
            "Loss 0.114\n",
            "Draw 0.132\n",
            "Evaluation after 40000 games\n",
            "Win 0.778\n",
            "Loss 0.126\n",
            "Draw 0.096\n",
            "Evaluation after 50000 games\n",
            "Win 0.788\n",
            "Loss 0.124\n",
            "Draw 0.088\n",
            "Evaluation after 60000 games\n",
            "Win 0.801\n",
            "Loss 0.103\n",
            "Draw 0.096\n",
            "Evaluation after 70000 games\n",
            "Win 0.806\n",
            "Loss 0.111\n",
            "Draw 0.083\n",
            "Evaluation after 80000 games\n",
            "Win 0.809\n",
            "Loss 0.12\n",
            "Draw 0.071\n",
            "Evaluation after 90000 games\n",
            "Win 0.823\n",
            "Loss 0.118\n",
            "Draw 0.059\n",
            "Evaluation after 100000 games\n",
            "Win 0.847\n",
            "Loss 0.091\n",
            "Draw 0.062\n",
            "Evaluation after 110000 games\n",
            "Win 0.816\n",
            "Loss 0.118\n",
            "Draw 0.066\n",
            "Evaluation after 120000 games\n",
            "Win 0.839\n",
            "Loss 0.11\n",
            "Draw 0.051\n",
            "Evaluation after 130000 games\n",
            "Win 0.833\n",
            "Loss 0.113\n",
            "Draw 0.054\n",
            "Evaluation after 140000 games\n",
            "Win 0.842\n",
            "Loss 0.105\n",
            "Draw 0.053\n",
            "Evaluation after 150000 games\n",
            "Win 0.833\n",
            "Loss 0.117\n",
            "Draw 0.05\n",
            "Evaluation after 160000 games\n",
            "Win 0.857\n",
            "Loss 0.099\n",
            "Draw 0.044\n",
            "Evaluation after 170000 games\n",
            "Win 0.871\n",
            "Loss 0.089\n",
            "Draw 0.04\n",
            "Evaluation after 180000 games\n",
            "Win 0.841\n",
            "Loss 0.11\n",
            "Draw 0.049\n",
            "Evaluation after 190000 games\n",
            "Win 0.836\n",
            "Loss 0.112\n",
            "Draw 0.052\n",
            "Evaluation after 200000 games\n",
            "Win 0.864\n",
            "Loss 0.092\n",
            "Draw 0.044\n",
            "Evaluation after 210000 games\n",
            "Win 0.853\n",
            "Loss 0.095\n",
            "Draw 0.052\n",
            "Evaluation after 220000 games\n",
            "Win 0.855\n",
            "Loss 0.103\n",
            "Draw 0.042\n",
            "Evaluation after 230000 games\n",
            "Win 0.853\n",
            "Loss 0.102\n",
            "Draw 0.045\n",
            "Evaluation after 240000 games\n",
            "Win 0.867\n",
            "Loss 0.092\n",
            "Draw 0.041\n",
            "Evaluation after 250000 games\n",
            "Win 0.865\n",
            "Loss 0.094\n",
            "Draw 0.041\n",
            "Evaluation after 260000 games\n",
            "Win 0.89\n",
            "Loss 0.078\n",
            "Draw 0.032\n",
            "Evaluation after 270000 games\n",
            "Win 0.876\n",
            "Loss 0.093\n",
            "Draw 0.031\n",
            "Evaluation after 280000 games\n",
            "Win 0.867\n",
            "Loss 0.092\n",
            "Draw 0.041\n",
            "Evaluation after 290000 games\n",
            "Win 0.891\n",
            "Loss 0.079\n",
            "Draw 0.03\n",
            "Evaluation after 300000 games\n",
            "Win 0.879\n",
            "Loss 0.088\n",
            "Draw 0.033\n",
            "Evaluation after 310000 games\n",
            "Win 0.866\n",
            "Loss 0.099\n",
            "Draw 0.035\n",
            "Evaluation after 320000 games\n",
            "Win 0.887\n",
            "Loss 0.087\n",
            "Draw 0.026\n",
            "Evaluation after 330000 games\n",
            "Win 0.879\n",
            "Loss 0.089\n",
            "Draw 0.032\n",
            "Evaluation after 340000 games\n",
            "Win 0.896\n",
            "Loss 0.083\n",
            "Draw 0.021\n",
            "Evaluation after 350000 games\n",
            "Win 0.91\n",
            "Loss 0.071\n",
            "Draw 0.019\n",
            "Evaluation after 360000 games\n",
            "Win 0.896\n",
            "Loss 0.082\n",
            "Draw 0.022\n",
            "Evaluation after 370000 games\n",
            "Win 0.883\n",
            "Loss 0.086\n",
            "Draw 0.031\n",
            "Evaluation after 380000 games\n",
            "Win 0.89\n",
            "Loss 0.088\n",
            "Draw 0.022\n",
            "Evaluation after 390000 games\n",
            "Win 0.885\n",
            "Loss 0.087\n",
            "Draw 0.028\n",
            "Evaluation after 400000 games\n",
            "Win 0.907\n",
            "Loss 0.075\n",
            "Draw 0.018\n",
            "Evaluation after 410000 games\n",
            "Win 0.9\n",
            "Loss 0.08\n",
            "Draw 0.02\n",
            "Evaluation after 420000 games\n",
            "Win 0.902\n",
            "Loss 0.087\n",
            "Draw 0.011\n",
            "Evaluation after 430000 games\n",
            "Win 0.906\n",
            "Loss 0.08\n",
            "Draw 0.014\n",
            "Evaluation after 440000 games\n",
            "Win 0.902\n",
            "Loss 0.073\n",
            "Draw 0.025\n",
            "Evaluation after 450000 games\n",
            "Win 0.897\n",
            "Loss 0.085\n",
            "Draw 0.018\n",
            "Evaluation after 460000 games\n",
            "Win 0.896\n",
            "Loss 0.094\n",
            "Draw 0.01\n",
            "Evaluation after 470000 games\n",
            "Win 0.919\n",
            "Loss 0.066\n",
            "Draw 0.015\n",
            "Evaluation after 480000 games\n",
            "Win 0.916\n",
            "Loss 0.071\n",
            "Draw 0.013\n",
            "Evaluation after 490000 games\n",
            "Win 0.908\n",
            "Loss 0.078\n",
            "Draw 0.014\n",
            "Evaluation after 1000 games\n",
            "Win 0.906\n",
            "Loss 0.083\n",
            "Draw 0.011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(player_1.model.state_dict(),\"/content/weights.pt\")\n",
        "torch.save(v_net.state_dict(),\"/content/v_net.pt\")"
      ],
      "metadata": {
        "id": "GjXoVJNb2Ln7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r runs_vanilla /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "yAktvrqBwQ5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "player_1.eval=True\n",
        "loss = 0\n",
        "draw = 0\n",
        "win = 0\n",
        "for i in range(1000):\n",
        "    _,reward,_ = games(player=\"mini\")\n",
        "    if(reward==1.0):\n",
        "      win+=1\n",
        "    elif(reward==-1.0):\n",
        "      loss+=1\n",
        "    else:\n",
        "      draw+=1\n",
        "\n",
        "\n",
        "\n",
        "writer.add_scalar(\"Win_percentage_strong\", win*100)\n",
        "writer.add_scalar(\"Loss_percentage_strong\",loss*100)\n",
        "writer.add_scalar(\"Draw_percentage_strong\",draw*100)\n",
        "print(\"Evaluation after\",playouts,\"games\")\n",
        "print(\"Win\",win*100/10)\n",
        "print(\"Loss\",loss*100/10)\n",
        "print(\"Draw\",draw*100/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ml8CnVzo6pLs",
        "outputId": "0b51c4c7-7581-433b-d9cf-1a9dcfdeae07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-c46244972755>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mini\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mwin\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-e46a3f9a3ea3>\u001b[0m in \u001b[0;36mgames\u001b[0;34m(player)\u001b[0m\n\u001b[1;32m     23\u001b[0m        \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mget_move\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# X's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# O's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# X's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# O's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# X's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# O's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# X's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-1f9de2935207>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(self, state, depth, is_maximizing)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# O's move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(win,draw,loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umo3ncUUAg7E",
        "outputId": "1a824868-702a-40c5-f9e3-89c17b7b8b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132 31 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tictac-master.zip -d ./tmp\n",
        "!mv ./tmp/*/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TO7eXJn9ElO",
        "outputId": "48e37c35-858d-4804-eaa8-b0675674d32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tictac-master.zip\n",
            "220bbdc6103ff012ec60b5b424e1566205349588\n",
            "   creating: ./tmp/tictac-master/\n",
            "  inflating: ./tmp/tictac-master/.gitignore  \n",
            "  inflating: ./tmp/tictac-master/Pipfile  \n",
            "  inflating: ./tmp/tictac-master/Pipfile.lock  \n",
            "  inflating: ./tmp/tictac-master/README.md  \n",
            "  inflating: ./tmp/tictac-master/demo_get_average_values.py  \n",
            "  inflating: ./tmp/tictac-master/path.bat  \n",
            "  inflating: ./tmp/tictac-master/path.sh  \n",
            "   creating: ./tmp/tictac-master/tests/\n",
            "  inflating: ./tmp/tictac-master/tests/test_board.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_mcts.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_minimax.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_qneural.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_qtable.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_random.py  \n",
            "  inflating: ./tmp/tictac-master/tests/test_transform.py  \n",
            "   creating: ./tmp/tictac-master/tictac/\n",
            "  inflating: ./tmp/tictac-master/tictac/board.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/main.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/main_qneural.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/mcts.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/minimax.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/qneural.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/qtable.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/qtable_old.py  \n",
            "  inflating: ./tmp/tictac-master/tictac/transform.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r tmp"
      ],
      "metadata": {
        "id": "DI9K9h149Js-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tictac.minimax import play_minimax_move\n",
        "_,move = play_minimax_move(board.state,True)\n",
        "print(board.state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "jJtM4Nqe9efu",
        "outputId": "96b9d4db-354a-4c6d-c2e9-a7922ce4eded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-7532f0f8e225>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtictac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplay_minimax_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_minimax_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tictac/minimax.py\u001b[0m in \u001b[0;36mplay_minimax_move\u001b[0;34m(board, randomize)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_minimax_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmove_value_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_move_value_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_best_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_value_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tictac/minimax.py\u001b[0m in \u001b[0;36mget_move_value_pairs\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_move_value_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvalid_move_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid_move_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_move_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"never call with an end position\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'get_valid_move_indexes'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}