{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9VgXfyfjZME",
        "outputId": "1d5fd1bd-3255-4e2a-e9f6-170fe62ca630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wins: 1831, Losses: 105, Ties: 64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Define the policy network using an MLP\n",
        "class MLPPolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLPPolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        output = torch.softmax(self.fc2(x), dim=-1)\n",
        "        return output\n",
        "\n",
        "# Function to select an action based on the policy network's output probabilities\n",
        "def select_action(policy_net, state):\n",
        "    action_probs = policy_net(state)\n",
        "    action_dist = Categorical(action_probs)\n",
        "    action = action_dist.sample()\n",
        "    return action.item()\n",
        "\n",
        "# Function to update the policy network based on the REINFORCE algorithm\n",
        "def reinforce_update(policy_net, saved_log_probs, rewards, optimizer):\n",
        "    policy_loss = []\n",
        "    for log_prob, reward in zip(saved_log_probs, rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Function to play a round of Rock Paper Scissors against the biased opponent\n",
        "def play_round(action):\n",
        "    p = 0.9 # Probability for the first action\n",
        "    q = random.random() * (1 - p)  # Random probability for the second action\n",
        "    opponent_action_probs = [p, q, 1 - p - q]  # Biased probabilities for Rock, Paper, Scissors\n",
        "    opponent_action = random.choices([0, 1, 2], weights=opponent_action_probs)[0]\n",
        "\n",
        "    reward = get_reward(action, opponent_action)\n",
        "\n",
        "    return opponent_action, reward, True\n",
        "\n",
        "# Function to determine the reward based on the chosen action and opponent's action\n",
        "def get_reward(action, opponent_action):\n",
        "    if (action == 0 and opponent_action == 2) or (action == 1 and opponent_action == 0) or (action == 2 and opponent_action == 1):\n",
        "        return 1  # Win\n",
        "    elif action == opponent_action:\n",
        "        return 0  # Tie\n",
        "    else:\n",
        "        return -1  # Lose\n",
        "\n",
        "# Function to train the policy network using the REINFORCE algorithm\n",
        "def train_policy_network():\n",
        "    input_size = 3  # Number of possible actions (Rock, Paper, Scissors)\n",
        "    hidden_size = 128\n",
        "    output_size = 3\n",
        "\n",
        "    policy_net = MLPPolicyNetwork(input_size, hidden_size, output_size)\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "\n",
        "    num_episodes = 100000\n",
        "    for episode in range(num_episodes):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            state = torch.ones(1, input_size)  # Input state is a tensor of ones\n",
        "            action = select_action(policy_net, state)\n",
        "            opponent_action, reward, done = play_round(action)\n",
        "            saved_log_probs.append(torch.log(policy_net(state))[0][action])\n",
        "            rewards.append(reward)\n",
        "\n",
        "        reinforce_update(policy_net, saved_log_probs, rewards, optimizer)\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# Train the policy network\n",
        "policy_network = train_policy_network()\n",
        "\n",
        "# Test the policy network\n",
        "num_trials = 2000\n",
        "wins = 0\n",
        "losses = 0\n",
        "ties = 0\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    input_size = 3\n",
        "    state = torch.ones(1, input_size)\n",
        "    action = select_action(policy_network, state)\n",
        "    opponent_action, reward, _ = play_round(action)\n",
        "\n",
        "    if reward == 1:\n",
        "        wins += 1\n",
        "    elif reward == -1:\n",
        "        losses += 1\n",
        "    else:\n",
        "        ties += 1\n",
        "\n",
        "print(f\"Wins: {wins}, Losses: {losses}, Ties: {ties}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCkwfYMH0_Qa",
        "outputId": "52474e8a-976a-4c6b-f832-ae72ce214e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7 41 17 12\n",
            "0.8 47 17 6\n",
            "0.9 53 11 6\n",
            "0.3 34 21 15\n",
            "0.5 42 12 16\n",
            "0.2 37 21 12\n",
            "0.4 33 23 14\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define the policy network using an MLP\n",
        "class MLPPolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLPPolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        output = torch.softmax(self.fc2(x), dim=-1)\n",
        "        dist = Categorical(probs=output)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        return action,log_prob\n",
        "\n",
        "# Function to select an action based on the policy network's output probabilities\n",
        "def select_action(policy_net, state):\n",
        "    action_probs = policy_net(state)\n",
        "    action_dist = Categorical(action_probs)\n",
        "    action = action_dist.sample()\n",
        "    return action.item()\n",
        "\n",
        "# Function to update the policy network based on the REINFORCE algorithm\n",
        "def reinforce_update(policy_net, saved_log_probs, rewards, optimizer):\n",
        "    policy_loss = []\n",
        "    for log_prob, reward in zip(saved_log_probs, rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    return policy_loss\n",
        "\n",
        "# Function to play a round of Rock Paper Scissors against the biased opponent\n",
        "def play_round(action, p):\n",
        "    q = random.random() * (1 - p)  # Random probability for the second action\n",
        "    opponent_action_probs = [p, q, 1 - p - q]  # Biased probabilities for Rock, Paper, Scissors\n",
        "    opponent_action = random.choices([0, 1, 2], weights=opponent_action_probs)[0]\n",
        "\n",
        "    reward = get_reward(action, opponent_action)\n",
        "\n",
        "    return opponent_action, reward, True\n",
        "\n",
        "# Function to determine the reward based on the chosen action and opponent's action\n",
        "def get_reward(action, opponent_action):\n",
        "    if (action == 0 and opponent_action == 2) or (action == 1 and opponent_action == 0) or (action == 2 and opponent_action == 1):\n",
        "        return 1  # Win\n",
        "    elif action == opponent_action:\n",
        "        return 0  # Tie\n",
        "    else:\n",
        "        return -1  # Lose\n",
        "\n",
        "# Function to train the policy network using the MAML algorithm\n",
        "def train_policy_network(p_values):\n",
        "    input_size = 3  # Number of possible actions (Rock, Paper, Scissors)\n",
        "    hidden_size = 128\n",
        "    output_size = 3\n",
        "    num_inner_updates = 1  # Number of inner updates in the MAML algorithm\n",
        "    num_episodes = 10000\n",
        "    policy_net = MLPPolicyNetwork(input_size, hidden_size, output_size)\n",
        "    meta_optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "    for episode in range(num_episodes):\n",
        "            # Initialize the meta-optimizer with the current policy network parameters\n",
        "            meta_optimizer.zero_grad()\n",
        "            meta_loss = 0\n",
        "            losses = []\n",
        "            for p in p_values:\n",
        "              for _ in range(num_inner_updates):\n",
        "                  # Clone the policy network for the inner update\n",
        "                  policy_net_clone = MLPPolicyNetwork(input_size, hidden_size, output_size)\n",
        "                  policy_net_clone.load_state_dict(policy_net.state_dict())\n",
        "                  saved_log_probs = []\n",
        "                  rewards = []\n",
        "\n",
        "                  state,_,_ = play_round(1,p)  # Task action\n",
        "                  inp = F.one_hot(torch.tensor(state),3).float()\n",
        "                  out,log_prob = policy_net_clone(inp)\n",
        "                  reward = get_reward(out.item(),state)\n",
        "                  loss = -log_prob*reward\n",
        "                  loss.backward(create_graph=True)\n",
        "                  for parameter in policy_net_clone.parameters():\n",
        "                     parameter.data-= 0.01*parameter.grad\n",
        "                  state,_,_ = play_round(1,p)\n",
        "                  inp = F.one_hot(torch.tensor(state),3).float()\n",
        "                  out,log_prob = policy_net_clone(inp)\n",
        "                  reward = get_reward(out,state)\n",
        "                  loss = -log_prob*reward\n",
        "                  losses.append(loss)\n",
        "              meta_loss = torch.sum(torch.stack(losses))/len(losses)\n",
        "              meta_loss.backward(retain_graph=True)\n",
        "              meta_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# Train the policy network for different initial bias values\n",
        "p_values = [0.3, 0.5, 0.7,0.8,0.9,0.32,0.45]\n",
        "#policy_network = train_policy_network(p_values)\n",
        "\n",
        "# Test the policy network\n",
        "num_trials = 70\n",
        "p_values = [0.7,0.8,0.9,0.3,0.5,0.2,0.4]\n",
        "wins = {p: 0 for p in p_values}\n",
        "losses = {p: 0 for p in p_values}\n",
        "ties = {p: 0 for p in p_values}\n",
        "for p in p_values:\n",
        "     p_net = MLPPolicyNetwork(3, 128, 3)\n",
        "     opt = torch.optim.Adam(p_net.parameters(),1e-3)\n",
        "     p_net.load_state_dict(policy_network.state_dict())\n",
        "     for _ in range(num_trials):\n",
        "        state,_,_ = play_round(1,p)  # Task action\n",
        "        inp = F.one_hot(torch.tensor(state),3).float()\n",
        "        out,log_prob = p_net(inp)\n",
        "        reward = get_reward(out.item(),state)\n",
        "        loss = -log_prob*reward\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "     ##Test ##\n",
        "     wins=0\n",
        "     loss=0\n",
        "     draw =0\n",
        "     for _ in range(num_trials):\n",
        "        state,_,_ = play_round(1,p)  # Task action\n",
        "        inp = F.one_hot(torch.tensor(state),3).float()\n",
        "        out,log_prob = p_net(inp)\n",
        "        reward = get_reward(out.item(),state)\n",
        "        if(reward==1):\n",
        "          wins+=1\n",
        "        elif(reward==0):\n",
        "          draw+=1\n",
        "        else:\n",
        "          loss+=1\n",
        "     print(p,wins,draw,loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h2aQtvGFkzIe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the moves\n",
        "rock = 0\n",
        "paper = 1\n",
        "scissors = 2\n",
        "\n",
        "# Define the outcome matrix\n",
        "outcome_matrix = torch.tensor([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
        "\n",
        "class RPS_MLP_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(RPS_MLP_LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len,_ = x.size()\n",
        "        h0 = torch.zeros(1, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(1, self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the RPS MLP with LSTM model\n",
        "input_dim = 3\n",
        "hidden_dim = 32\n",
        "output_dim = 3\n",
        "model = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the REINFORCE training function\n",
        "def train_reinforce(model, num_episodes, sequence_length, lr):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        # Generate a single sequence of RPS moves with skewed probabilities\n",
        "        probabilities = torch.tensor([0.9, 0.1, 0.0])  # Probabilities for rock, paper, scissors\n",
        "        input_sequence = torch.multinomial(probabilities, sequence_length, replacement=True)\n",
        "        input_one_hot = F.one_hot(input_sequence,3)\n",
        "        #print(input_one_hot)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize the hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros(1, hidden_dim)\n",
        "        c0 = torch.zeros(1, hidden_dim)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "        outputs = model(input_one_hot.float())\n",
        "        print(outputs.shape)\n",
        "        m = Categorical(torch.softmax(outputs, dim=-1))\n",
        "\n",
        "\n",
        "            # Sample an action\n",
        "        action_ = m.sample()\n",
        "        log_prob = m.log_prob(action_)\n",
        "        #print(action_)\n",
        "\n",
        "\n",
        "        # Compute the reward for each output\n",
        "        for i in range(sequence_length):\n",
        "            # Compute the action probabilities from the output\n",
        "\n",
        "            action = action_[i]\n",
        "\n",
        "            if(action ==input_sequence[i]):\n",
        "              reward = 0.0\n",
        "            elif(action==0 and input_sequence[i]==1):\n",
        "              reward = -1.0\n",
        "            elif(action==0 and input_sequence[i]==2):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==1 and input_sequence[i]==2):\n",
        "              reward = -1.0\n",
        "            elif(action==1 and input_sequence[i]==0):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==2 and input_sequence[i]==0):\n",
        "              reward = -1.0\n",
        "            elif(action==2 and input_sequence[i]==1):\n",
        "              reward = 1.0\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the reward based on the sampled action and actual move\n",
        "\n",
        "\n",
        "            # Store the log probability and reward\n",
        "            log_probs.append(log_prob[i])\n",
        "            rewards.append(reward)\n",
        "\n",
        "\n",
        "        # Compute the cumulative rewards\n",
        "        cum_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r + cumulative_reward\n",
        "            cum_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        # Compute the loss and update the model\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        cum_rewards = torch.tensor(cum_rewards)\n",
        "        loss = -torch.sum(log_probs * cum_rewards)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss for monitoring\n",
        "        print(\"Episode: {}, Loss: {}\".format(episode+1, loss.item()))\n",
        "\n",
        "# Example usage\n",
        "num_episodes = 10000\n",
        "sequence_length = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_reinforce(model, num_episodes, sequence_length, learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0ycpOQNwRvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cca31b8-d2f6-439a-b787-7afd325059d0",
        "id": "QXOInZRkwSRY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Loss: 87.02294158935547\n",
            "Episode: 2, Loss: 138.2305145263672\n",
            "Episode: 3, Loss: 94.677978515625\n",
            "Episode: 4, Loss: 12.0185546875\n",
            "Episode: 5, Loss: 159.50355529785156\n",
            "Episode: 6, Loss: -36.04782485961914\n",
            "Episode: 7, Loss: 90.3535385131836\n",
            "Episode: 8, Loss: -68.0043716430664\n",
            "Episode: 9, Loss: 119.5509033203125\n",
            "Episode: 10, Loss: 341.01055908203125\n",
            "Episode: 11, Loss: -121.37286376953125\n",
            "Episode: 12, Loss: 319.4160461425781\n",
            "Episode: 13, Loss: 38.41069412231445\n",
            "Episode: 14, Loss: -407.9892578125\n",
            "Episode: 15, Loss: 17.099451065063477\n",
            "Episode: 16, Loss: 258.1130065917969\n",
            "Episode: 17, Loss: -379.1492004394531\n",
            "Episode: 18, Loss: -54.5765495300293\n",
            "Episode: 19, Loss: 162.23150634765625\n",
            "Episode: 20, Loss: 5.848731994628906\n",
            "Episode: 21, Loss: -54.19862747192383\n",
            "Episode: 22, Loss: -49.76011276245117\n",
            "Episode: 23, Loss: 136.52735900878906\n",
            "Episode: 24, Loss: -370.3114013671875\n",
            "Episode: 25, Loss: 470.4056396484375\n",
            "Episode: 26, Loss: 132.26548767089844\n",
            "Episode: 27, Loss: 454.7788391113281\n",
            "Episode: 28, Loss: -30.82596778869629\n",
            "Episode: 29, Loss: 199.9163360595703\n",
            "Episode: 30, Loss: 82.43828582763672\n",
            "Episode: 31, Loss: -63.134098052978516\n",
            "Episode: 32, Loss: -152.46839904785156\n",
            "Episode: 33, Loss: -244.4710693359375\n",
            "Episode: 34, Loss: -11.118896484375\n",
            "Episode: 35, Loss: 119.11002349853516\n",
            "Episode: 36, Loss: -192.85888671875\n",
            "Episode: 37, Loss: 108.3457260131836\n",
            "Episode: 38, Loss: 357.0218200683594\n",
            "Episode: 39, Loss: -48.891143798828125\n",
            "Episode: 40, Loss: 125.51628875732422\n",
            "Episode: 41, Loss: 423.23876953125\n",
            "Episode: 42, Loss: 186.3921661376953\n",
            "Episode: 43, Loss: -3.1654865741729736\n",
            "Episode: 44, Loss: -104.5925064086914\n",
            "Episode: 45, Loss: 102.086669921875\n",
            "Episode: 46, Loss: 421.3041687011719\n",
            "Episode: 47, Loss: 22.366689682006836\n",
            "Episode: 48, Loss: 47.570064544677734\n",
            "Episode: 49, Loss: 60.92059326171875\n",
            "Episode: 50, Loss: -294.6861877441406\n",
            "Episode: 51, Loss: -145.94862365722656\n",
            "Episode: 52, Loss: 185.7021484375\n",
            "Episode: 53, Loss: 233.5086212158203\n",
            "Episode: 54, Loss: 112.19792938232422\n",
            "Episode: 55, Loss: 107.79205322265625\n",
            "Episode: 56, Loss: -244.0927276611328\n",
            "Episode: 57, Loss: -23.0218448638916\n",
            "Episode: 58, Loss: -15.564971923828125\n",
            "Episode: 59, Loss: 154.93690490722656\n",
            "Episode: 60, Loss: -99.00617218017578\n",
            "Episode: 61, Loss: 59.4339485168457\n",
            "Episode: 62, Loss: -103.20086669921875\n",
            "Episode: 63, Loss: 126.691650390625\n",
            "Episode: 64, Loss: 69.58479309082031\n",
            "Episode: 65, Loss: -331.8816833496094\n",
            "Episode: 66, Loss: -139.4152069091797\n",
            "Episode: 67, Loss: 200.3791961669922\n",
            "Episode: 68, Loss: 26.052133560180664\n",
            "Episode: 69, Loss: -208.9691619873047\n",
            "Episode: 70, Loss: -50.436405181884766\n",
            "Episode: 71, Loss: -125.1236572265625\n",
            "Episode: 72, Loss: 256.0564880371094\n",
            "Episode: 73, Loss: 77.13742065429688\n",
            "Episode: 74, Loss: 162.7021942138672\n",
            "Episode: 75, Loss: 306.5677185058594\n",
            "Episode: 76, Loss: 21.822021484375\n",
            "Episode: 77, Loss: -209.0876007080078\n",
            "Episode: 78, Loss: 131.2798309326172\n",
            "Episode: 79, Loss: 159.8243408203125\n",
            "Episode: 80, Loss: -249.8180389404297\n",
            "Episode: 81, Loss: 226.608154296875\n",
            "Episode: 82, Loss: -115.54117584228516\n",
            "Episode: 83, Loss: -55.29719161987305\n",
            "Episode: 84, Loss: 66.27832794189453\n",
            "Episode: 85, Loss: 16.93658447265625\n",
            "Episode: 86, Loss: 248.8507080078125\n",
            "Episode: 87, Loss: 261.4919128417969\n",
            "Episode: 88, Loss: -424.5389709472656\n",
            "Episode: 89, Loss: -7.266976833343506\n",
            "Episode: 90, Loss: -177.5728302001953\n",
            "Episode: 91, Loss: 218.97552490234375\n",
            "Episode: 92, Loss: -229.9867706298828\n",
            "Episode: 93, Loss: 183.7491912841797\n",
            "Episode: 94, Loss: 216.4271240234375\n",
            "Episode: 95, Loss: 32.45552444458008\n",
            "Episode: 96, Loss: -351.6814270019531\n",
            "Episode: 97, Loss: -449.4049072265625\n",
            "Episode: 98, Loss: -61.14088439941406\n",
            "Episode: 99, Loss: 401.205322265625\n",
            "Episode: 100, Loss: 197.59002685546875\n",
            "Episode: 101, Loss: 101.9134750366211\n",
            "Episode: 102, Loss: -114.80045318603516\n",
            "Episode: 103, Loss: 293.9436340332031\n",
            "Episode: 104, Loss: -187.90321350097656\n",
            "Episode: 105, Loss: -179.3402862548828\n",
            "Episode: 106, Loss: -15.723666191101074\n",
            "Episode: 107, Loss: -476.7216796875\n",
            "Episode: 108, Loss: 98.65472412109375\n",
            "Episode: 109, Loss: -293.3880615234375\n",
            "Episode: 110, Loss: -28.546524047851562\n",
            "Episode: 111, Loss: -242.89256286621094\n",
            "Episode: 112, Loss: 8.693984985351562\n",
            "Episode: 113, Loss: 45.96915817260742\n",
            "Episode: 114, Loss: -13.287002563476562\n",
            "Episode: 115, Loss: 194.19541931152344\n",
            "Episode: 116, Loss: -544.1565551757812\n",
            "Episode: 117, Loss: -7.517110347747803\n",
            "Episode: 118, Loss: 90.88802337646484\n",
            "Episode: 119, Loss: 236.94911193847656\n",
            "Episode: 120, Loss: -141.4914093017578\n",
            "Episode: 121, Loss: -7.503488063812256\n",
            "Episode: 122, Loss: -94.85873413085938\n",
            "Episode: 123, Loss: -101.99486541748047\n",
            "Episode: 124, Loss: 224.23448181152344\n",
            "Episode: 125, Loss: 192.69696044921875\n",
            "Episode: 126, Loss: -104.37395477294922\n",
            "Episode: 127, Loss: 131.17787170410156\n",
            "Episode: 128, Loss: -245.20460510253906\n",
            "Episode: 129, Loss: -41.26710891723633\n",
            "Episode: 130, Loss: -343.3639221191406\n",
            "Episode: 131, Loss: 192.6671905517578\n",
            "Episode: 132, Loss: -72.95792388916016\n",
            "Episode: 133, Loss: 246.2528839111328\n",
            "Episode: 134, Loss: 365.7063903808594\n",
            "Episode: 135, Loss: 30.234933853149414\n",
            "Episode: 136, Loss: -32.5062141418457\n",
            "Episode: 137, Loss: 35.119754791259766\n",
            "Episode: 138, Loss: 18.2674503326416\n",
            "Episode: 139, Loss: 75.19558715820312\n",
            "Episode: 140, Loss: -447.6846923828125\n",
            "Episode: 141, Loss: -259.291259765625\n",
            "Episode: 142, Loss: -45.58158493041992\n",
            "Episode: 143, Loss: 19.86360740661621\n",
            "Episode: 144, Loss: -395.812744140625\n",
            "Episode: 145, Loss: 88.13030242919922\n",
            "Episode: 146, Loss: -212.43995666503906\n",
            "Episode: 147, Loss: 152.95191955566406\n",
            "Episode: 148, Loss: -0.1384989470243454\n",
            "Episode: 149, Loss: 333.6533508300781\n",
            "Episode: 150, Loss: -75.1104965209961\n",
            "Episode: 151, Loss: 13.9666748046875\n",
            "Episode: 152, Loss: 90.6589584350586\n",
            "Episode: 153, Loss: 357.0686950683594\n",
            "Episode: 154, Loss: -98.52591705322266\n",
            "Episode: 155, Loss: 44.8250617980957\n",
            "Episode: 156, Loss: 51.99787521362305\n",
            "Episode: 157, Loss: -70.93460083007812\n",
            "Episode: 158, Loss: 243.5467987060547\n",
            "Episode: 159, Loss: -12.091415405273438\n",
            "Episode: 160, Loss: -200.8345184326172\n",
            "Episode: 161, Loss: -51.05836868286133\n",
            "Episode: 162, Loss: -426.0674743652344\n",
            "Episode: 163, Loss: -40.376182556152344\n",
            "Episode: 164, Loss: 288.0893249511719\n",
            "Episode: 165, Loss: 131.1006622314453\n",
            "Episode: 166, Loss: 42.64225387573242\n",
            "Episode: 167, Loss: -158.54922485351562\n",
            "Episode: 168, Loss: -79.98162841796875\n",
            "Episode: 169, Loss: 473.4634704589844\n",
            "Episode: 170, Loss: 106.29097747802734\n",
            "Episode: 171, Loss: 29.933996200561523\n",
            "Episode: 172, Loss: -77.44657897949219\n",
            "Episode: 173, Loss: -397.9854736328125\n",
            "Episode: 174, Loss: 115.45934295654297\n",
            "Episode: 175, Loss: -266.35186767578125\n",
            "Episode: 176, Loss: -61.9485969543457\n",
            "Episode: 177, Loss: 270.4226379394531\n",
            "Episode: 178, Loss: -131.70716857910156\n",
            "Episode: 179, Loss: -274.4448547363281\n",
            "Episode: 180, Loss: 10.349601745605469\n",
            "Episode: 181, Loss: 220.14869689941406\n",
            "Episode: 182, Loss: 156.8663787841797\n",
            "Episode: 183, Loss: -200.96905517578125\n",
            "Episode: 184, Loss: -46.84354782104492\n",
            "Episode: 185, Loss: 26.39802360534668\n",
            "Episode: 186, Loss: -129.4725341796875\n",
            "Episode: 187, Loss: 546.1615600585938\n",
            "Episode: 188, Loss: 249.45347595214844\n",
            "Episode: 189, Loss: 80.89336395263672\n",
            "Episode: 190, Loss: -125.71932983398438\n",
            "Episode: 191, Loss: 92.5002670288086\n",
            "Episode: 192, Loss: 5.859583377838135\n",
            "Episode: 193, Loss: 374.7772216796875\n",
            "Episode: 194, Loss: 225.2384490966797\n",
            "Episode: 195, Loss: -128.6166534423828\n",
            "Episode: 196, Loss: -100.5239486694336\n",
            "Episode: 197, Loss: -126.9771957397461\n",
            "Episode: 198, Loss: 533.3753051757812\n",
            "Episode: 199, Loss: -190.7874298095703\n",
            "Episode: 200, Loss: -7.0286407470703125\n",
            "Episode: 201, Loss: 3.07739520072937\n",
            "Episode: 202, Loss: 190.98936462402344\n",
            "Episode: 203, Loss: -425.6033630371094\n",
            "Episode: 204, Loss: -370.0302734375\n",
            "Episode: 205, Loss: -195.2230987548828\n",
            "Episode: 206, Loss: 67.525146484375\n",
            "Episode: 207, Loss: 85.71002197265625\n",
            "Episode: 208, Loss: 197.98316955566406\n",
            "Episode: 209, Loss: 208.69725036621094\n",
            "Episode: 210, Loss: 170.16688537597656\n",
            "Episode: 211, Loss: 72.23400115966797\n",
            "Episode: 212, Loss: 277.9901428222656\n",
            "Episode: 213, Loss: -25.3240966796875\n",
            "Episode: 214, Loss: -25.727500915527344\n",
            "Episode: 215, Loss: 196.6588134765625\n",
            "Episode: 216, Loss: -327.8062438964844\n",
            "Episode: 217, Loss: 42.86994552612305\n",
            "Episode: 218, Loss: -99.73016357421875\n",
            "Episode: 219, Loss: 251.84703063964844\n",
            "Episode: 220, Loss: -324.3857727050781\n",
            "Episode: 221, Loss: -51.99587631225586\n",
            "Episode: 222, Loss: -8.999415397644043\n",
            "Episode: 223, Loss: -144.59820556640625\n",
            "Episode: 224, Loss: -213.1703338623047\n",
            "Episode: 225, Loss: -354.7995300292969\n",
            "Episode: 226, Loss: -222.63975524902344\n",
            "Episode: 227, Loss: -373.94482421875\n",
            "Episode: 228, Loss: -30.29827308654785\n",
            "Episode: 229, Loss: -67.94635009765625\n",
            "Episode: 230, Loss: 203.5973663330078\n",
            "Episode: 231, Loss: -93.69478607177734\n",
            "Episode: 232, Loss: -3.26446533203125\n",
            "Episode: 233, Loss: 265.8695373535156\n",
            "Episode: 234, Loss: 316.3233947753906\n",
            "Episode: 235, Loss: 10.079803466796875\n",
            "Episode: 236, Loss: -260.3872375488281\n",
            "Episode: 237, Loss: -82.53536987304688\n",
            "Episode: 238, Loss: -151.23326110839844\n",
            "Episode: 239, Loss: -174.762451171875\n",
            "Episode: 240, Loss: -209.4144287109375\n",
            "Episode: 241, Loss: -206.5209503173828\n",
            "Episode: 242, Loss: 223.94554138183594\n",
            "Episode: 243, Loss: -215.16943359375\n",
            "Episode: 244, Loss: 118.98281860351562\n",
            "Episode: 245, Loss: 172.9814453125\n",
            "Episode: 246, Loss: -284.72869873046875\n",
            "Episode: 247, Loss: -35.637359619140625\n",
            "Episode: 248, Loss: -144.126708984375\n",
            "Episode: 249, Loss: -58.1510124206543\n",
            "Episode: 250, Loss: -173.24546813964844\n",
            "Episode: 251, Loss: 273.2518615722656\n",
            "Episode: 252, Loss: 308.8517761230469\n",
            "Episode: 253, Loss: -112.43408203125\n",
            "Episode: 254, Loss: 61.52522659301758\n",
            "Episode: 255, Loss: -10.228861808776855\n",
            "Episode: 256, Loss: -237.9003448486328\n",
            "Episode: 257, Loss: 197.676513671875\n",
            "Episode: 258, Loss: -196.833984375\n",
            "Episode: 259, Loss: 287.2567443847656\n",
            "Episode: 260, Loss: -88.01046752929688\n",
            "Episode: 261, Loss: 81.78559112548828\n",
            "Episode: 262, Loss: -40.53693771362305\n",
            "Episode: 263, Loss: 33.3519172668457\n",
            "Episode: 264, Loss: 19.831832885742188\n",
            "Episode: 265, Loss: -538.3034057617188\n",
            "Episode: 266, Loss: -610.6387939453125\n",
            "Episode: 267, Loss: -57.17670822143555\n",
            "Episode: 268, Loss: 449.6014404296875\n",
            "Episode: 269, Loss: 209.3703155517578\n",
            "Episode: 270, Loss: 267.850830078125\n",
            "Episode: 271, Loss: -197.0639190673828\n",
            "Episode: 272, Loss: 131.5601348876953\n",
            "Episode: 273, Loss: -187.4619903564453\n",
            "Episode: 274, Loss: -37.956695556640625\n",
            "Episode: 275, Loss: 366.9571228027344\n",
            "Episode: 276, Loss: 123.06192016601562\n",
            "Episode: 277, Loss: -213.01068115234375\n",
            "Episode: 278, Loss: -418.1116638183594\n",
            "Episode: 279, Loss: -261.3407287597656\n",
            "Episode: 280, Loss: -21.730560302734375\n",
            "Episode: 281, Loss: -154.91773986816406\n",
            "Episode: 282, Loss: -345.0158996582031\n",
            "Episode: 283, Loss: 233.0858917236328\n",
            "Episode: 284, Loss: 67.14591217041016\n",
            "Episode: 285, Loss: 66.38265991210938\n",
            "Episode: 286, Loss: -11.68317699432373\n",
            "Episode: 287, Loss: 15.889195442199707\n",
            "Episode: 288, Loss: 10.16375732421875\n",
            "Episode: 289, Loss: 154.340576171875\n",
            "Episode: 290, Loss: 79.27637481689453\n",
            "Episode: 291, Loss: -84.13652801513672\n",
            "Episode: 292, Loss: 153.3761749267578\n",
            "Episode: 293, Loss: 159.00205993652344\n",
            "Episode: 294, Loss: 390.6402893066406\n",
            "Episode: 295, Loss: 143.7095947265625\n",
            "Episode: 296, Loss: 7.501913547515869\n",
            "Episode: 297, Loss: -229.8695831298828\n",
            "Episode: 298, Loss: -12.300804138183594\n",
            "Episode: 299, Loss: -133.75701904296875\n",
            "Episode: 300, Loss: 57.91549301147461\n",
            "Episode: 301, Loss: 122.7447509765625\n",
            "Episode: 302, Loss: 107.09786987304688\n",
            "Episode: 303, Loss: 93.53067016601562\n",
            "Episode: 304, Loss: 264.8900451660156\n",
            "Episode: 305, Loss: -56.87385940551758\n",
            "Episode: 306, Loss: -15.0404052734375\n",
            "Episode: 307, Loss: -65.05248260498047\n",
            "Episode: 308, Loss: 354.9826965332031\n",
            "Episode: 309, Loss: -82.13017272949219\n",
            "Episode: 310, Loss: -75.9051513671875\n",
            "Episode: 311, Loss: 26.390832901000977\n",
            "Episode: 312, Loss: -10.632405281066895\n",
            "Episode: 313, Loss: -45.3791389465332\n",
            "Episode: 314, Loss: -158.5734405517578\n",
            "Episode: 315, Loss: 176.92576599121094\n",
            "Episode: 316, Loss: 132.5302276611328\n",
            "Episode: 317, Loss: -81.9023208618164\n",
            "Episode: 318, Loss: -87.01467895507812\n",
            "Episode: 319, Loss: -223.900634765625\n",
            "Episode: 320, Loss: -31.588403701782227\n",
            "Episode: 321, Loss: 208.4017791748047\n",
            "Episode: 322, Loss: 151.3618621826172\n",
            "Episode: 323, Loss: 175.255126953125\n",
            "Episode: 324, Loss: 603.6431274414062\n",
            "Episode: 325, Loss: -158.7288360595703\n",
            "Episode: 326, Loss: -127.62096405029297\n",
            "Episode: 327, Loss: -200.98382568359375\n",
            "Episode: 328, Loss: 35.94578170776367\n",
            "Episode: 329, Loss: 117.55591583251953\n",
            "Episode: 330, Loss: 448.8595886230469\n",
            "Episode: 331, Loss: -209.2215576171875\n",
            "Episode: 332, Loss: 209.6228485107422\n",
            "Episode: 333, Loss: -23.866363525390625\n",
            "Episode: 334, Loss: -182.33770751953125\n",
            "Episode: 335, Loss: 213.7272491455078\n",
            "Episode: 336, Loss: -196.0273895263672\n",
            "Episode: 337, Loss: -226.04888916015625\n",
            "Episode: 338, Loss: -145.1672821044922\n",
            "Episode: 339, Loss: -39.47867965698242\n",
            "Episode: 340, Loss: -262.0970153808594\n",
            "Episode: 341, Loss: -303.1099548339844\n",
            "Episode: 342, Loss: -213.8900146484375\n",
            "Episode: 343, Loss: 54.86405944824219\n",
            "Episode: 344, Loss: 165.30535888671875\n",
            "Episode: 345, Loss: -59.839996337890625\n",
            "Episode: 346, Loss: 128.07937622070312\n",
            "Episode: 347, Loss: -299.6374206542969\n",
            "Episode: 348, Loss: -137.472900390625\n",
            "Episode: 349, Loss: 134.6172332763672\n",
            "Episode: 350, Loss: -537.9785766601562\n",
            "Episode: 351, Loss: 121.5007553100586\n",
            "Episode: 352, Loss: 105.9360122680664\n",
            "Episode: 353, Loss: 84.90289306640625\n",
            "Episode: 354, Loss: -415.9430847167969\n",
            "Episode: 355, Loss: 11.3856782913208\n",
            "Episode: 356, Loss: -128.19589233398438\n",
            "Episode: 357, Loss: 87.72815704345703\n",
            "Episode: 358, Loss: 158.08982849121094\n",
            "Episode: 359, Loss: -163.9520721435547\n",
            "Episode: 360, Loss: 411.528076171875\n",
            "Episode: 361, Loss: -626.0491333007812\n",
            "Episode: 362, Loss: -4.50018835067749\n",
            "Episode: 363, Loss: -310.4909973144531\n",
            "Episode: 364, Loss: -29.13907814025879\n",
            "Episode: 365, Loss: -116.62793731689453\n",
            "Episode: 366, Loss: -153.90065002441406\n",
            "Episode: 367, Loss: 51.17257308959961\n",
            "Episode: 368, Loss: 150.25308227539062\n",
            "Episode: 369, Loss: 240.7942657470703\n",
            "Episode: 370, Loss: -403.795166015625\n",
            "Episode: 371, Loss: 134.3428192138672\n",
            "Episode: 372, Loss: 9.837016105651855\n",
            "Episode: 373, Loss: -359.2886047363281\n",
            "Episode: 374, Loss: -23.33542823791504\n",
            "Episode: 375, Loss: -198.5661163330078\n",
            "Episode: 376, Loss: -239.8634490966797\n",
            "Episode: 377, Loss: 222.5917510986328\n",
            "Episode: 378, Loss: -266.5390319824219\n",
            "Episode: 379, Loss: -9.946208000183105\n",
            "Episode: 380, Loss: 36.84006881713867\n",
            "Episode: 381, Loss: -134.96566772460938\n",
            "Episode: 382, Loss: -425.8189697265625\n",
            "Episode: 383, Loss: -405.9024963378906\n",
            "Episode: 384, Loss: 68.2650375366211\n",
            "Episode: 385, Loss: -148.6073760986328\n",
            "Episode: 386, Loss: -391.4077453613281\n",
            "Episode: 387, Loss: 75.85177612304688\n",
            "Episode: 388, Loss: 181.2836151123047\n",
            "Episode: 389, Loss: -414.9791564941406\n",
            "Episode: 390, Loss: 422.5249328613281\n",
            "Episode: 391, Loss: -375.8757629394531\n",
            "Episode: 392, Loss: -129.8402862548828\n",
            "Episode: 393, Loss: -179.26536560058594\n",
            "Episode: 394, Loss: 406.76953125\n",
            "Episode: 395, Loss: 21.37947654724121\n",
            "Episode: 396, Loss: 235.41705322265625\n",
            "Episode: 397, Loss: -273.50372314453125\n",
            "Episode: 398, Loss: 478.644775390625\n",
            "Episode: 399, Loss: -102.4654312133789\n",
            "Episode: 400, Loss: -12.729817390441895\n",
            "Episode: 401, Loss: -126.26729583740234\n",
            "Episode: 402, Loss: 205.92071533203125\n",
            "Episode: 403, Loss: 126.78026580810547\n",
            "Episode: 404, Loss: 69.6408920288086\n",
            "Episode: 405, Loss: 42.41640090942383\n",
            "Episode: 406, Loss: 134.43228149414062\n",
            "Episode: 407, Loss: -86.02136993408203\n",
            "Episode: 408, Loss: 310.4984436035156\n",
            "Episode: 409, Loss: 411.694580078125\n",
            "Episode: 410, Loss: 20.93715476989746\n",
            "Episode: 411, Loss: 114.6701431274414\n",
            "Episode: 412, Loss: -64.98487091064453\n",
            "Episode: 413, Loss: 118.43294525146484\n",
            "Episode: 414, Loss: 92.01671600341797\n",
            "Episode: 415, Loss: 274.9659423828125\n",
            "Episode: 416, Loss: 195.781982421875\n",
            "Episode: 417, Loss: 306.09295654296875\n",
            "Episode: 418, Loss: -81.66089630126953\n",
            "Episode: 419, Loss: -91.62332153320312\n",
            "Episode: 420, Loss: 302.4615783691406\n",
            "Episode: 421, Loss: -60.965118408203125\n",
            "Episode: 422, Loss: 10.063984870910645\n",
            "Episode: 423, Loss: 89.7437973022461\n",
            "Episode: 424, Loss: -16.26253318786621\n",
            "Episode: 425, Loss: 201.8583526611328\n",
            "Episode: 426, Loss: 308.21392822265625\n",
            "Episode: 427, Loss: 196.40806579589844\n",
            "Episode: 428, Loss: -223.8418731689453\n",
            "Episode: 429, Loss: -395.3404541015625\n",
            "Episode: 430, Loss: 42.0799446105957\n",
            "Episode: 431, Loss: -279.6490478515625\n",
            "Episode: 432, Loss: -149.5696258544922\n",
            "Episode: 433, Loss: 79.81421661376953\n",
            "Episode: 434, Loss: -69.5188217163086\n",
            "Episode: 435, Loss: 369.6936340332031\n",
            "Episode: 436, Loss: 69.7357177734375\n",
            "Episode: 437, Loss: 92.434326171875\n",
            "Episode: 438, Loss: 97.79376220703125\n",
            "Episode: 439, Loss: 279.78948974609375\n",
            "Episode: 440, Loss: 260.1859436035156\n",
            "Episode: 441, Loss: 361.3309631347656\n",
            "Episode: 442, Loss: 41.618038177490234\n",
            "Episode: 443, Loss: 61.79581832885742\n",
            "Episode: 444, Loss: 187.0489044189453\n",
            "Episode: 445, Loss: -387.5168151855469\n",
            "Episode: 446, Loss: -93.04257202148438\n",
            "Episode: 447, Loss: -146.0885009765625\n",
            "Episode: 448, Loss: 56.926361083984375\n",
            "Episode: 449, Loss: -229.8719940185547\n",
            "Episode: 450, Loss: 195.89573669433594\n",
            "Episode: 451, Loss: -315.4800720214844\n",
            "Episode: 452, Loss: 349.3096618652344\n",
            "Episode: 453, Loss: -26.126739501953125\n",
            "Episode: 454, Loss: -196.13548278808594\n",
            "Episode: 455, Loss: 234.4590301513672\n",
            "Episode: 456, Loss: 196.1240692138672\n",
            "Episode: 457, Loss: 218.10362243652344\n",
            "Episode: 458, Loss: -95.86260986328125\n",
            "Episode: 459, Loss: 9.208897590637207\n",
            "Episode: 460, Loss: -183.6197509765625\n",
            "Episode: 461, Loss: 2.0444743633270264\n",
            "Episode: 462, Loss: -15.708638191223145\n",
            "Episode: 463, Loss: -185.8592529296875\n",
            "Episode: 464, Loss: -264.0599670410156\n",
            "Episode: 465, Loss: -138.08119201660156\n",
            "Episode: 466, Loss: -362.4053649902344\n",
            "Episode: 467, Loss: 160.09280395507812\n",
            "Episode: 468, Loss: -303.293212890625\n",
            "Episode: 469, Loss: -27.004274368286133\n",
            "Episode: 470, Loss: -20.225067138671875\n",
            "Episode: 471, Loss: -142.19357299804688\n",
            "Episode: 472, Loss: -195.6097412109375\n",
            "Episode: 473, Loss: 212.03721618652344\n",
            "Episode: 474, Loss: -28.286163330078125\n",
            "Episode: 475, Loss: 157.9142303466797\n",
            "Episode: 476, Loss: 47.740570068359375\n",
            "Episode: 477, Loss: 131.4581298828125\n",
            "Episode: 478, Loss: -179.09326171875\n",
            "Episode: 479, Loss: 261.90185546875\n",
            "Episode: 480, Loss: -569.3810424804688\n",
            "Episode: 481, Loss: -86.46286010742188\n",
            "Episode: 482, Loss: -222.66099548339844\n",
            "Episode: 483, Loss: -249.986328125\n",
            "Episode: 484, Loss: 146.77760314941406\n",
            "Episode: 485, Loss: 601.8300170898438\n",
            "Episode: 486, Loss: -257.2544860839844\n",
            "Episode: 487, Loss: 197.4912109375\n",
            "Episode: 488, Loss: 272.5178527832031\n",
            "Episode: 489, Loss: -176.75506591796875\n",
            "Episode: 490, Loss: -6.782506465911865\n",
            "Episode: 491, Loss: 188.06101989746094\n",
            "Episode: 492, Loss: -267.8759460449219\n",
            "Episode: 493, Loss: 150.20384216308594\n",
            "Episode: 494, Loss: -27.30536460876465\n",
            "Episode: 495, Loss: 72.09681701660156\n",
            "Episode: 496, Loss: 350.0824890136719\n",
            "Episode: 497, Loss: -187.0541534423828\n",
            "Episode: 498, Loss: -171.76295471191406\n",
            "Episode: 499, Loss: -3.942621946334839\n",
            "Episode: 500, Loss: -70.95108795166016\n",
            "Episode: 501, Loss: 536.9017333984375\n",
            "Episode: 502, Loss: 290.0506896972656\n",
            "Episode: 503, Loss: 173.7901611328125\n",
            "Episode: 504, Loss: 341.95751953125\n",
            "Episode: 505, Loss: 64.83001708984375\n",
            "Episode: 506, Loss: 197.6088409423828\n",
            "Episode: 507, Loss: -293.4021911621094\n",
            "Episode: 508, Loss: 436.9786071777344\n",
            "Episode: 509, Loss: -267.9163818359375\n",
            "Episode: 510, Loss: 157.72584533691406\n",
            "Episode: 511, Loss: -187.29856872558594\n",
            "Episode: 512, Loss: 67.82100677490234\n",
            "Episode: 513, Loss: 54.73591232299805\n",
            "Episode: 514, Loss: 44.71427917480469\n",
            "Episode: 515, Loss: 276.5544128417969\n",
            "Episode: 516, Loss: 121.86654663085938\n",
            "Episode: 517, Loss: 128.78248596191406\n",
            "Episode: 518, Loss: 89.8209228515625\n",
            "Episode: 519, Loss: 185.85255432128906\n",
            "Episode: 520, Loss: 283.11688232421875\n",
            "Episode: 521, Loss: 385.1799011230469\n",
            "Episode: 522, Loss: -359.6875\n",
            "Episode: 523, Loss: 169.52053833007812\n",
            "Episode: 524, Loss: -148.0148162841797\n",
            "Episode: 525, Loss: -101.79186248779297\n",
            "Episode: 526, Loss: -45.61698532104492\n",
            "Episode: 527, Loss: -21.59160804748535\n",
            "Episode: 528, Loss: -454.4831848144531\n",
            "Episode: 529, Loss: 193.3935089111328\n",
            "Episode: 530, Loss: -315.9801330566406\n",
            "Episode: 531, Loss: 79.35665130615234\n",
            "Episode: 532, Loss: -73.83348846435547\n",
            "Episode: 533, Loss: -186.7848663330078\n",
            "Episode: 534, Loss: -239.0110321044922\n",
            "Episode: 535, Loss: 28.710525512695312\n",
            "Episode: 536, Loss: 1.0588277578353882\n",
            "Episode: 537, Loss: 13.79804515838623\n",
            "Episode: 538, Loss: -129.77197265625\n",
            "Episode: 539, Loss: -17.311391830444336\n",
            "Episode: 540, Loss: 2.1541011333465576\n",
            "Episode: 541, Loss: 49.77241516113281\n",
            "Episode: 542, Loss: -271.4008483886719\n",
            "Episode: 543, Loss: 48.297786712646484\n",
            "Episode: 544, Loss: -182.10862731933594\n",
            "Episode: 545, Loss: -52.700286865234375\n",
            "Episode: 546, Loss: 160.1525421142578\n",
            "Episode: 547, Loss: 55.331722259521484\n",
            "Episode: 548, Loss: -214.14414978027344\n",
            "Episode: 549, Loss: -352.2752685546875\n",
            "Episode: 550, Loss: -451.6064758300781\n",
            "Episode: 551, Loss: -264.9089660644531\n",
            "Episode: 552, Loss: 455.0357360839844\n",
            "Episode: 553, Loss: 98.1111068725586\n",
            "Episode: 554, Loss: -76.0264892578125\n",
            "Episode: 555, Loss: -315.3930358886719\n",
            "Episode: 556, Loss: 79.35234069824219\n",
            "Episode: 557, Loss: -61.3017692565918\n",
            "Episode: 558, Loss: 114.2440185546875\n",
            "Episode: 559, Loss: 210.79701232910156\n",
            "Episode: 560, Loss: 100.95462036132812\n",
            "Episode: 561, Loss: -66.70671844482422\n",
            "Episode: 562, Loss: 175.95068359375\n",
            "Episode: 563, Loss: 96.187255859375\n",
            "Episode: 564, Loss: 116.11297607421875\n",
            "Episode: 565, Loss: -258.9443664550781\n",
            "Episode: 566, Loss: 57.37969970703125\n",
            "Episode: 567, Loss: -399.6523132324219\n",
            "Episode: 568, Loss: -147.425048828125\n",
            "Episode: 569, Loss: 155.2073211669922\n",
            "Episode: 570, Loss: -30.0357723236084\n",
            "Episode: 571, Loss: 12.906909942626953\n",
            "Episode: 572, Loss: -368.7980041503906\n",
            "Episode: 573, Loss: 10.07977294921875\n",
            "Episode: 574, Loss: 49.60344314575195\n",
            "Episode: 575, Loss: 175.3636474609375\n",
            "Episode: 576, Loss: -274.8677062988281\n",
            "Episode: 577, Loss: -96.89420318603516\n",
            "Episode: 578, Loss: 207.805419921875\n",
            "Episode: 579, Loss: 132.9302215576172\n",
            "Episode: 580, Loss: 282.9892883300781\n",
            "Episode: 581, Loss: 84.06184387207031\n",
            "Episode: 582, Loss: 2.5168838500976562\n",
            "Episode: 583, Loss: 96.88534545898438\n",
            "Episode: 584, Loss: 0.7772267460823059\n",
            "Episode: 585, Loss: 50.525177001953125\n",
            "Episode: 586, Loss: 76.40149688720703\n",
            "Episode: 587, Loss: 64.54309844970703\n",
            "Episode: 588, Loss: 32.6402587890625\n",
            "Episode: 589, Loss: -216.0422821044922\n",
            "Episode: 590, Loss: -647.1121826171875\n",
            "Episode: 591, Loss: -362.372802734375\n",
            "Episode: 592, Loss: 44.50770950317383\n",
            "Episode: 593, Loss: -242.30589294433594\n",
            "Episode: 594, Loss: -154.0867919921875\n",
            "Episode: 595, Loss: -232.6768035888672\n",
            "Episode: 596, Loss: 317.4421081542969\n",
            "Episode: 597, Loss: 111.73316192626953\n",
            "Episode: 598, Loss: -13.19329833984375\n",
            "Episode: 599, Loss: 848.7304077148438\n",
            "Episode: 600, Loss: 21.82466697692871\n",
            "Episode: 601, Loss: -27.249032974243164\n",
            "Episode: 602, Loss: -30.8033447265625\n",
            "Episode: 603, Loss: -481.8033142089844\n",
            "Episode: 604, Loss: 138.42724609375\n",
            "Episode: 605, Loss: -16.341873168945312\n",
            "Episode: 606, Loss: 46.91911315917969\n",
            "Episode: 607, Loss: -232.7791290283203\n",
            "Episode: 608, Loss: -20.17841148376465\n",
            "Episode: 609, Loss: 23.70686912536621\n",
            "Episode: 610, Loss: -19.919248580932617\n",
            "Episode: 611, Loss: -178.5110626220703\n",
            "Episode: 612, Loss: 58.48879623413086\n",
            "Episode: 613, Loss: -139.94442749023438\n",
            "Episode: 614, Loss: -88.94173431396484\n",
            "Episode: 615, Loss: 12.690815925598145\n",
            "Episode: 616, Loss: -227.8511962890625\n",
            "Episode: 617, Loss: 125.12481689453125\n",
            "Episode: 618, Loss: -42.12697982788086\n",
            "Episode: 619, Loss: -15.396740913391113\n",
            "Episode: 620, Loss: 302.0364685058594\n",
            "Episode: 621, Loss: 76.79736328125\n",
            "Episode: 622, Loss: 183.86669921875\n",
            "Episode: 623, Loss: -183.54766845703125\n",
            "Episode: 624, Loss: -109.14026641845703\n",
            "Episode: 625, Loss: -20.56389808654785\n",
            "Episode: 626, Loss: 104.94867706298828\n",
            "Episode: 627, Loss: -593.6204223632812\n",
            "Episode: 628, Loss: 17.83931541442871\n",
            "Episode: 629, Loss: 71.91521453857422\n",
            "Episode: 630, Loss: -104.805419921875\n",
            "Episode: 631, Loss: 282.6112976074219\n",
            "Episode: 632, Loss: 293.1730651855469\n",
            "Episode: 633, Loss: 60.1024055480957\n",
            "Episode: 634, Loss: 122.685302734375\n",
            "Episode: 635, Loss: 442.2418518066406\n",
            "Episode: 636, Loss: -63.7053108215332\n",
            "Episode: 637, Loss: -245.1868438720703\n",
            "Episode: 638, Loss: -528.9887084960938\n",
            "Episode: 639, Loss: 221.3480224609375\n",
            "Episode: 640, Loss: 150.31576538085938\n",
            "Episode: 641, Loss: -316.8080749511719\n",
            "Episode: 642, Loss: -18.40749168395996\n",
            "Episode: 643, Loss: -287.9947509765625\n",
            "Episode: 644, Loss: -47.438873291015625\n",
            "Episode: 645, Loss: -98.41128540039062\n",
            "Episode: 646, Loss: 216.54949951171875\n",
            "Episode: 647, Loss: 20.379140853881836\n",
            "Episode: 648, Loss: 16.682479858398438\n",
            "Episode: 649, Loss: 243.7223358154297\n",
            "Episode: 650, Loss: 119.3172607421875\n",
            "Episode: 651, Loss: -158.67494201660156\n",
            "Episode: 652, Loss: 592.9962768554688\n",
            "Episode: 653, Loss: 192.7811737060547\n",
            "Episode: 654, Loss: 34.71345901489258\n",
            "Episode: 655, Loss: -327.0151062011719\n",
            "Episode: 656, Loss: 99.07880401611328\n",
            "Episode: 657, Loss: -125.30368041992188\n",
            "Episode: 658, Loss: 121.89151763916016\n",
            "Episode: 659, Loss: 78.0069580078125\n",
            "Episode: 660, Loss: 49.878509521484375\n",
            "Episode: 661, Loss: 167.26641845703125\n",
            "Episode: 662, Loss: -7.298891544342041\n",
            "Episode: 663, Loss: 113.03759765625\n",
            "Episode: 664, Loss: -68.75304412841797\n",
            "Episode: 665, Loss: -123.7187271118164\n",
            "Episode: 666, Loss: 110.40289306640625\n",
            "Episode: 667, Loss: 73.04851531982422\n",
            "Episode: 668, Loss: -298.2433166503906\n",
            "Episode: 669, Loss: 123.77912139892578\n",
            "Episode: 670, Loss: -12.051457405090332\n",
            "Episode: 671, Loss: -52.85462951660156\n",
            "Episode: 672, Loss: -129.37322998046875\n",
            "Episode: 673, Loss: 351.2947998046875\n",
            "Episode: 674, Loss: 267.1408386230469\n",
            "Episode: 675, Loss: 117.07880401611328\n",
            "Episode: 676, Loss: -254.91552734375\n",
            "Episode: 677, Loss: 100.23062896728516\n",
            "Episode: 678, Loss: 336.5345458984375\n",
            "Episode: 679, Loss: -58.12466049194336\n",
            "Episode: 680, Loss: 147.41229248046875\n",
            "Episode: 681, Loss: -61.04350662231445\n",
            "Episode: 682, Loss: 76.58053588867188\n",
            "Episode: 683, Loss: -147.25425720214844\n",
            "Episode: 684, Loss: -99.26000213623047\n",
            "Episode: 685, Loss: 120.23505401611328\n",
            "Episode: 686, Loss: 396.0013122558594\n",
            "Episode: 687, Loss: -206.9475860595703\n",
            "Episode: 688, Loss: 64.59973907470703\n",
            "Episode: 689, Loss: -136.52732849121094\n",
            "Episode: 690, Loss: -26.410415649414062\n",
            "Episode: 691, Loss: 153.79100036621094\n",
            "Episode: 692, Loss: -49.50002670288086\n",
            "Episode: 693, Loss: 191.94573974609375\n",
            "Episode: 694, Loss: 217.03257751464844\n",
            "Episode: 695, Loss: 98.01941680908203\n",
            "Episode: 696, Loss: -192.4888458251953\n",
            "Episode: 697, Loss: -204.537109375\n",
            "Episode: 698, Loss: 122.19573974609375\n",
            "Episode: 699, Loss: -170.4732208251953\n",
            "Episode: 700, Loss: -221.9064178466797\n",
            "Episode: 701, Loss: -101.98152923583984\n",
            "Episode: 702, Loss: 364.5785217285156\n",
            "Episode: 703, Loss: 103.98086547851562\n",
            "Episode: 704, Loss: -130.43807983398438\n",
            "Episode: 705, Loss: -212.07969665527344\n",
            "Episode: 706, Loss: -309.877685546875\n",
            "Episode: 707, Loss: -246.39947509765625\n",
            "Episode: 708, Loss: -219.99888610839844\n",
            "Episode: 709, Loss: 284.8504638671875\n",
            "Episode: 710, Loss: -158.22940063476562\n",
            "Episode: 711, Loss: -161.23545837402344\n",
            "Episode: 712, Loss: -7.170827388763428\n",
            "Episode: 713, Loss: 15.853118896484375\n",
            "Episode: 714, Loss: -217.11431884765625\n",
            "Episode: 715, Loss: 34.157352447509766\n",
            "Episode: 716, Loss: -260.7492980957031\n",
            "Episode: 717, Loss: 127.13967895507812\n",
            "Episode: 718, Loss: -72.9070053100586\n",
            "Episode: 719, Loss: -229.450439453125\n",
            "Episode: 720, Loss: -191.8011016845703\n",
            "Episode: 721, Loss: -348.9359436035156\n",
            "Episode: 722, Loss: -32.211483001708984\n",
            "Episode: 723, Loss: 20.609033584594727\n",
            "Episode: 724, Loss: -134.5809326171875\n",
            "Episode: 725, Loss: 349.13720703125\n",
            "Episode: 726, Loss: -237.5576934814453\n",
            "Episode: 727, Loss: 165.27085876464844\n",
            "Episode: 728, Loss: -265.1632995605469\n",
            "Episode: 729, Loss: -25.68119239807129\n",
            "Episode: 730, Loss: -236.1721649169922\n",
            "Episode: 731, Loss: -82.08440399169922\n",
            "Episode: 732, Loss: 10.711527824401855\n",
            "Episode: 733, Loss: -181.7140655517578\n",
            "Episode: 734, Loss: -144.49581909179688\n",
            "Episode: 735, Loss: -94.69731903076172\n",
            "Episode: 736, Loss: -113.9890365600586\n",
            "Episode: 737, Loss: -305.4245910644531\n",
            "Episode: 738, Loss: -106.56137084960938\n",
            "Episode: 739, Loss: -105.09140014648438\n",
            "Episode: 740, Loss: -23.16249656677246\n",
            "Episode: 741, Loss: 128.1664276123047\n",
            "Episode: 742, Loss: 70.09735870361328\n",
            "Episode: 743, Loss: 254.72412109375\n",
            "Episode: 744, Loss: -217.9542694091797\n",
            "Episode: 745, Loss: 89.2623291015625\n",
            "Episode: 746, Loss: 41.00227737426758\n",
            "Episode: 747, Loss: 240.4940185546875\n",
            "Episode: 748, Loss: 117.9340591430664\n",
            "Episode: 749, Loss: -77.77238464355469\n",
            "Episode: 750, Loss: -41.3397216796875\n",
            "Episode: 751, Loss: 408.0193176269531\n",
            "Episode: 752, Loss: 298.01116943359375\n",
            "Episode: 753, Loss: 307.9286193847656\n",
            "Episode: 754, Loss: 159.86473083496094\n",
            "Episode: 755, Loss: 154.977783203125\n",
            "Episode: 756, Loss: -461.5799560546875\n",
            "Episode: 757, Loss: -273.9165954589844\n",
            "Episode: 758, Loss: 23.1699275970459\n",
            "Episode: 759, Loss: 171.7633514404297\n",
            "Episode: 760, Loss: 30.22322654724121\n",
            "Episode: 761, Loss: 302.3080749511719\n",
            "Episode: 762, Loss: 24.1546630859375\n",
            "Episode: 763, Loss: 105.42513275146484\n",
            "Episode: 764, Loss: -181.7459259033203\n",
            "Episode: 765, Loss: -328.3356628417969\n",
            "Episode: 766, Loss: 517.5241088867188\n",
            "Episode: 767, Loss: 88.17398071289062\n",
            "Episode: 768, Loss: 33.39866638183594\n",
            "Episode: 769, Loss: -92.17813873291016\n",
            "Episode: 770, Loss: -58.37824630737305\n",
            "Episode: 771, Loss: -162.48504638671875\n",
            "Episode: 772, Loss: -368.2596435546875\n",
            "Episode: 773, Loss: 338.4290771484375\n",
            "Episode: 774, Loss: 96.65143585205078\n",
            "Episode: 775, Loss: -77.7437973022461\n",
            "Episode: 776, Loss: 104.750244140625\n",
            "Episode: 777, Loss: 35.3155632019043\n",
            "Episode: 778, Loss: -125.58563232421875\n",
            "Episode: 779, Loss: -150.71633911132812\n",
            "Episode: 780, Loss: 62.84490966796875\n",
            "Episode: 781, Loss: 98.7257080078125\n",
            "Episode: 782, Loss: -256.5385437011719\n",
            "Episode: 783, Loss: -141.51068115234375\n",
            "Episode: 784, Loss: 106.04345703125\n",
            "Episode: 785, Loss: 102.67514038085938\n",
            "Episode: 786, Loss: -177.4930419921875\n",
            "Episode: 787, Loss: 270.0037841796875\n",
            "Episode: 788, Loss: -4.1071600914001465\n",
            "Episode: 789, Loss: -11.518234252929688\n",
            "Episode: 790, Loss: -329.0298767089844\n",
            "Episode: 791, Loss: 153.33616638183594\n",
            "Episode: 792, Loss: -481.7384033203125\n",
            "Episode: 793, Loss: 147.1930389404297\n",
            "Episode: 794, Loss: -204.8039093017578\n",
            "Episode: 795, Loss: 205.22589111328125\n",
            "Episode: 796, Loss: -82.6827163696289\n",
            "Episode: 797, Loss: 182.73146057128906\n",
            "Episode: 798, Loss: -306.8392028808594\n",
            "Episode: 799, Loss: -162.9817657470703\n",
            "Episode: 800, Loss: 183.00807189941406\n",
            "Episode: 801, Loss: 148.13377380371094\n",
            "Episode: 802, Loss: 233.1419219970703\n",
            "Episode: 803, Loss: -557.933837890625\n",
            "Episode: 804, Loss: -85.46382904052734\n",
            "Episode: 805, Loss: -72.38240814208984\n",
            "Episode: 806, Loss: -20.033666610717773\n",
            "Episode: 807, Loss: 9.230283737182617\n",
            "Episode: 808, Loss: 324.3913879394531\n",
            "Episode: 809, Loss: 291.47198486328125\n",
            "Episode: 810, Loss: 23.20033073425293\n",
            "Episode: 811, Loss: 24.696603775024414\n",
            "Episode: 812, Loss: 289.8586120605469\n",
            "Episode: 813, Loss: -267.4225769042969\n",
            "Episode: 814, Loss: -23.429067611694336\n",
            "Episode: 815, Loss: -97.51077270507812\n",
            "Episode: 816, Loss: 163.2061309814453\n",
            "Episode: 817, Loss: 160.11380004882812\n",
            "Episode: 818, Loss: -47.4459228515625\n",
            "Episode: 819, Loss: 121.39424896240234\n",
            "Episode: 820, Loss: -313.33807373046875\n",
            "Episode: 821, Loss: 202.9292755126953\n",
            "Episode: 822, Loss: -95.77032470703125\n",
            "Episode: 823, Loss: 272.0555114746094\n",
            "Episode: 824, Loss: 112.83992767333984\n",
            "Episode: 825, Loss: 97.2544937133789\n",
            "Episode: 826, Loss: -503.142578125\n",
            "Episode: 827, Loss: 60.24184036254883\n",
            "Episode: 828, Loss: -65.75997161865234\n",
            "Episode: 829, Loss: 160.88104248046875\n",
            "Episode: 830, Loss: -77.50650787353516\n",
            "Episode: 831, Loss: 42.76312255859375\n",
            "Episode: 832, Loss: -157.3961639404297\n",
            "Episode: 833, Loss: 82.8109359741211\n",
            "Episode: 834, Loss: -40.860198974609375\n",
            "Episode: 835, Loss: -148.9875030517578\n",
            "Episode: 836, Loss: 461.8770446777344\n",
            "Episode: 837, Loss: 2.3400802612304688\n",
            "Episode: 838, Loss: -163.84573364257812\n",
            "Episode: 839, Loss: -156.97850036621094\n",
            "Episode: 840, Loss: 381.8528747558594\n",
            "Episode: 841, Loss: -154.5255889892578\n",
            "Episode: 842, Loss: -7.7799248695373535\n",
            "Episode: 843, Loss: 407.8221130371094\n",
            "Episode: 844, Loss: -407.5853576660156\n",
            "Episode: 845, Loss: -25.666940689086914\n",
            "Episode: 846, Loss: 318.934326171875\n",
            "Episode: 847, Loss: -195.114013671875\n",
            "Episode: 848, Loss: -280.8717346191406\n",
            "Episode: 849, Loss: -80.07696533203125\n",
            "Episode: 850, Loss: -195.71998596191406\n",
            "Episode: 851, Loss: -567.0300903320312\n",
            "Episode: 852, Loss: 36.31271743774414\n",
            "Episode: 853, Loss: -417.0894470214844\n",
            "Episode: 854, Loss: -9.25860595703125\n",
            "Episode: 855, Loss: -28.894189834594727\n",
            "Episode: 856, Loss: -3.5782878398895264\n",
            "Episode: 857, Loss: -659.8117065429688\n",
            "Episode: 858, Loss: -22.488515853881836\n",
            "Episode: 859, Loss: 196.605224609375\n",
            "Episode: 860, Loss: -430.1101989746094\n",
            "Episode: 861, Loss: -23.688203811645508\n",
            "Episode: 862, Loss: 57.9630241394043\n",
            "Episode: 863, Loss: 83.67891693115234\n",
            "Episode: 864, Loss: -349.4211730957031\n",
            "Episode: 865, Loss: 162.3719940185547\n",
            "Episode: 866, Loss: -132.12738037109375\n",
            "Episode: 867, Loss: -105.94855499267578\n",
            "Episode: 868, Loss: -350.2726745605469\n",
            "Episode: 869, Loss: 10.864990234375\n",
            "Episode: 870, Loss: -182.32264709472656\n",
            "Episode: 871, Loss: -365.44189453125\n",
            "Episode: 872, Loss: -183.45445251464844\n",
            "Episode: 873, Loss: -299.0360107421875\n",
            "Episode: 874, Loss: 67.2755126953125\n",
            "Episode: 875, Loss: 10.094223022460938\n",
            "Episode: 876, Loss: -238.5104522705078\n",
            "Episode: 877, Loss: 170.59063720703125\n",
            "Episode: 878, Loss: 90.2657241821289\n",
            "Episode: 879, Loss: -52.55402755737305\n",
            "Episode: 880, Loss: -549.0706176757812\n",
            "Episode: 881, Loss: 159.30242919921875\n",
            "Episode: 882, Loss: -17.06475830078125\n",
            "Episode: 883, Loss: -206.48313903808594\n",
            "Episode: 884, Loss: -23.877716064453125\n",
            "Episode: 885, Loss: -4.097910404205322\n",
            "Episode: 886, Loss: -3.754969358444214\n",
            "Episode: 887, Loss: 101.77740478515625\n",
            "Episode: 888, Loss: -45.957393646240234\n",
            "Episode: 889, Loss: -240.42662048339844\n",
            "Episode: 890, Loss: 180.5823974609375\n",
            "Episode: 891, Loss: -70.27928161621094\n",
            "Episode: 892, Loss: -57.68912124633789\n",
            "Episode: 893, Loss: -208.92771911621094\n",
            "Episode: 894, Loss: 141.46156311035156\n",
            "Episode: 895, Loss: 27.26350975036621\n",
            "Episode: 896, Loss: 9.594660758972168\n",
            "Episode: 897, Loss: 90.2633056640625\n",
            "Episode: 898, Loss: -56.7552604675293\n",
            "Episode: 899, Loss: 340.5677185058594\n",
            "Episode: 900, Loss: 55.383880615234375\n",
            "Episode: 901, Loss: -310.9527893066406\n",
            "Episode: 902, Loss: 16.777359008789062\n",
            "Episode: 903, Loss: 95.74993896484375\n",
            "Episode: 904, Loss: -347.4664001464844\n",
            "Episode: 905, Loss: 12.065945625305176\n",
            "Episode: 906, Loss: 87.2873306274414\n",
            "Episode: 907, Loss: -6.408762454986572\n",
            "Episode: 908, Loss: -34.84931945800781\n",
            "Episode: 909, Loss: -50.406795501708984\n",
            "Episode: 910, Loss: 386.002685546875\n",
            "Episode: 911, Loss: -223.4534149169922\n",
            "Episode: 912, Loss: 219.89088439941406\n",
            "Episode: 913, Loss: 18.800065994262695\n",
            "Episode: 914, Loss: -125.7071762084961\n",
            "Episode: 915, Loss: 303.9618225097656\n",
            "Episode: 916, Loss: 247.4350128173828\n",
            "Episode: 917, Loss: 168.69947814941406\n",
            "Episode: 918, Loss: 157.56581115722656\n",
            "Episode: 919, Loss: -286.0924377441406\n",
            "Episode: 920, Loss: 152.0824737548828\n",
            "Episode: 921, Loss: 92.97576904296875\n",
            "Episode: 922, Loss: 279.0751953125\n",
            "Episode: 923, Loss: -29.697675704956055\n",
            "Episode: 924, Loss: -253.7419891357422\n",
            "Episode: 925, Loss: 367.6644592285156\n",
            "Episode: 926, Loss: -431.6302490234375\n",
            "Episode: 927, Loss: -162.88058471679688\n",
            "Episode: 928, Loss: -34.744869232177734\n",
            "Episode: 929, Loss: 178.88218688964844\n",
            "Episode: 930, Loss: -106.5367202758789\n",
            "Episode: 931, Loss: 481.9979248046875\n",
            "Episode: 932, Loss: -94.66719818115234\n",
            "Episode: 933, Loss: 146.6346435546875\n",
            "Episode: 934, Loss: 135.08042907714844\n",
            "Episode: 935, Loss: -35.85697555541992\n",
            "Episode: 936, Loss: -119.7221450805664\n",
            "Episode: 937, Loss: -297.8930358886719\n",
            "Episode: 938, Loss: 32.72532653808594\n",
            "Episode: 939, Loss: 16.577194213867188\n",
            "Episode: 940, Loss: 40.983856201171875\n",
            "Episode: 941, Loss: 146.9353485107422\n",
            "Episode: 942, Loss: 348.6811828613281\n",
            "Episode: 943, Loss: -17.240991592407227\n",
            "Episode: 944, Loss: -531.3164672851562\n",
            "Episode: 945, Loss: 50.72567367553711\n",
            "Episode: 946, Loss: 34.5203971862793\n",
            "Episode: 947, Loss: -425.2842712402344\n",
            "Episode: 948, Loss: -139.4320831298828\n",
            "Episode: 949, Loss: -479.8424987792969\n",
            "Episode: 950, Loss: 177.2060546875\n",
            "Episode: 951, Loss: 205.51641845703125\n",
            "Episode: 952, Loss: -7.20026159286499\n",
            "Episode: 953, Loss: 119.66144561767578\n",
            "Episode: 954, Loss: 200.4712677001953\n",
            "Episode: 955, Loss: -97.24188232421875\n",
            "Episode: 956, Loss: -81.414794921875\n",
            "Episode: 957, Loss: -112.3266372680664\n",
            "Episode: 958, Loss: 175.1739044189453\n",
            "Episode: 959, Loss: 79.18656158447266\n",
            "Episode: 960, Loss: 334.1541748046875\n",
            "Episode: 961, Loss: -2.2323100566864014\n",
            "Episode: 962, Loss: -325.1609191894531\n",
            "Episode: 963, Loss: -96.96160888671875\n",
            "Episode: 964, Loss: 174.89599609375\n",
            "Episode: 965, Loss: 91.22750091552734\n",
            "Episode: 966, Loss: -77.12147521972656\n",
            "Episode: 967, Loss: 70.22566986083984\n",
            "Episode: 968, Loss: -144.8569793701172\n",
            "Episode: 969, Loss: -251.3509979248047\n",
            "Episode: 970, Loss: -55.32192611694336\n",
            "Episode: 971, Loss: 172.6094207763672\n",
            "Episode: 972, Loss: -106.05520629882812\n",
            "Episode: 973, Loss: 4.981638431549072\n",
            "Episode: 974, Loss: -60.05329513549805\n",
            "Episode: 975, Loss: -139.88941955566406\n",
            "Episode: 976, Loss: -21.5451717376709\n",
            "Episode: 977, Loss: 57.45029830932617\n",
            "Episode: 978, Loss: -312.25244140625\n",
            "Episode: 979, Loss: 226.5228271484375\n",
            "Episode: 980, Loss: -47.64640808105469\n",
            "Episode: 981, Loss: -294.5054016113281\n",
            "Episode: 982, Loss: 54.62117385864258\n",
            "Episode: 983, Loss: -53.73067092895508\n",
            "Episode: 984, Loss: -64.26681518554688\n",
            "Episode: 985, Loss: 249.7303009033203\n",
            "Episode: 986, Loss: 236.7845916748047\n",
            "Episode: 987, Loss: 28.6429443359375\n",
            "Episode: 988, Loss: 123.0538330078125\n",
            "Episode: 989, Loss: -116.61092376708984\n",
            "Episode: 990, Loss: -62.82919692993164\n",
            "Episode: 991, Loss: -70.20874786376953\n",
            "Episode: 992, Loss: 292.4056091308594\n",
            "Episode: 993, Loss: 8.17365550994873\n",
            "Episode: 994, Loss: -181.5872039794922\n",
            "Episode: 995, Loss: 242.4156494140625\n",
            "Episode: 996, Loss: -156.66323852539062\n",
            "Episode: 997, Loss: -150.88380432128906\n",
            "Episode: 998, Loss: -496.1966247558594\n",
            "Episode: 999, Loss: 9.734013557434082\n",
            "Episode: 1000, Loss: 238.9784393310547\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the moves\n",
        "rock = 0\n",
        "paper = 1\n",
        "scissors = 2\n",
        "\n",
        "# Define the outcome matrix\n",
        "outcome_matrix = torch.tensor([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
        "\n",
        "class RPS_MLP_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(RPS_MLP_LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len,_ = x.size()\n",
        "        h0 = torch.zeros(1, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(1, self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the RPS MLP with LSTM model\n",
        "input_dim = 3\n",
        "hidden_dim = 32\n",
        "output_dim = 3\n",
        "model = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the REINFORCE training function\n",
        "def train_reinforce(model, num_episodes, sequence_length, lr):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "      meta_loss = []\n",
        "      for probabilities in [torch.tensor([0.9, 0.1, 0.0]),torch.tensor([0.8, 0.1, 0.1]),torch.tensor([0.1, 0.9, 0.8]),torch.tensor([0.5, 0.5, 0.0]),torch.tensor([0.1, 0.1, 0.8]),torch.tensor([0.2, 0.1, 0.7])]:\n",
        "\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "\n",
        "        # Generate a single sequence of RPS moves with skewed probabilities\n",
        "          # Probabilities for rock, paper, scissors\n",
        "        input_sequence = torch.multinomial(probabilities, sequence_length, replacement=True)\n",
        "        input_one_hot = F.one_hot(input_sequence,3)\n",
        "        #print(input_one_hot)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize the hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros(1, hidden_dim)\n",
        "        c0 = torch.zeros(1, hidden_dim)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "        model_cloned = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "        model_cloned.load_state_dict(model.state_dict())\n",
        "        opt_cloned = torch.optim.SGD(model_cloned.parameters(),1e-2)  ## Not used\n",
        "\n",
        "        outputs = model_cloned(input_one_hot.float())\n",
        "\n",
        "        m = Categorical(torch.softmax(outputs, dim=-1))\n",
        "\n",
        "\n",
        "            # Sample an action\n",
        "        action_ = m.sample()\n",
        "        log_prob = m.log_prob(action_)\n",
        "        #print(action_)\n",
        "\n",
        "\n",
        "        # Compute the reward for each output\n",
        "        for i in range(sequence_length):\n",
        "            # Compute the action probabilities from the output\n",
        "\n",
        "            action = action_[i]\n",
        "\n",
        "            if(action ==input_sequence[i]):\n",
        "              reward = 0.0\n",
        "            elif(action==0 and input_sequence[i]==1):\n",
        "              reward = -1.0\n",
        "            elif(action==0 and input_sequence[i]==2):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==1 and input_sequence[i]==2):\n",
        "              reward = -1.0\n",
        "            elif(action==1 and input_sequence[i]==0):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==2 and input_sequence[i]==0):\n",
        "              reward = -1.0\n",
        "            elif(action==2 and input_sequence[i]==1):\n",
        "              reward = 1.0\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the reward based on the sampled action and actual move\n",
        "\n",
        "\n",
        "            # Store the log probability and reward\n",
        "            log_probs.append(log_prob[i])\n",
        "            rewards.append(reward)\n",
        "\n",
        "\n",
        "        # Compute the cumulative rewards\n",
        "        cum_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r + cumulative_reward\n",
        "            cum_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        # Compute the loss and update the model\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        cum_rewards = torch.tensor(cum_rewards)\n",
        "        loss = -torch.sum(log_probs * cum_rewards)\n",
        "        loss.backward(retain_graph=True)\n",
        "        for parameter in model_cloned.parameters():\n",
        "          parameter.data-= 0.01*parameter.grad\n",
        "        opt_cloned.zero_grad()\n",
        "\n",
        "        log_probs = []\n",
        "        rewards=[]\n",
        "        input_sequence = torch.multinomial(probabilities, sequence_length, replacement=True)\n",
        "        input_one_hot = F.one_hot(input_sequence,3)\n",
        "        #print(input_one_hot)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize the hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros(1, hidden_dim)\n",
        "        c0 = torch.zeros(1, hidden_dim)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "        model_cloned = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "        model_cloned.load_state_dict(model.state_dict())\n",
        "\n",
        "        outputs = model_cloned(input_one_hot.float())\n",
        "\n",
        "        m = Categorical(torch.softmax(outputs, dim=-1))\n",
        "\n",
        "\n",
        "            # Sample an action\n",
        "        action_ = m.sample()\n",
        "        log_prob = m.log_prob(action_)\n",
        "        #print(action_)\n",
        "\n",
        "\n",
        "        # Compute the reward for each output\n",
        "        for i in range(sequence_length):\n",
        "            # Compute the action probabilities from the output\n",
        "\n",
        "            action = action_[i]\n",
        "\n",
        "            if(action ==input_sequence[i]):\n",
        "              reward = 0.0\n",
        "            elif(action==0 and input_sequence[i]==1):\n",
        "              reward = -1.0\n",
        "            elif(action==0 and input_sequence[i]==2):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==1 and input_sequence[i]==2):\n",
        "              reward = -1.0\n",
        "            elif(action==1 and input_sequence[i]==0):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==2 and input_sequence[i]==0):\n",
        "              reward = -1.0\n",
        "            elif(action==2 and input_sequence[i]==1):\n",
        "              reward = 1.0\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the reward based on the sampled action and actual move\n",
        "\n",
        "\n",
        "            # Store the log probability and reward\n",
        "            log_probs.append(log_prob[i])\n",
        "            rewards.append(reward)\n",
        "\n",
        "\n",
        "        # Compute the cumulative rewards\n",
        "        cum_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r + cumulative_reward\n",
        "            cum_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        # Compute the loss and update the model\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        cum_rewards = torch.tensor(cum_rewards)\n",
        "        loss = -torch.sum(log_probs * cum_rewards)\n",
        "        meta_loss.append(loss)\n",
        "\n",
        "      loss_tot = torch.sum(torch.stack(meta_loss))/len(meta_loss)\n",
        "      optimizer.zero_grad()\n",
        "      loss_tot.backward()\n",
        "      optimizer.step()\n",
        "      print(\"Episode: {}, Loss: {}\".format(episode+1, loss_tot.item()))\n",
        "\n",
        "# Example usage\n",
        "num_episodes = 1000\n",
        "sequence_length = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_reinforce(model, num_episodes, sequence_length, learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "test_model = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "test_model.load_state_dict(model.state_dict())\n",
        "probabilities =torch.tensor([1.0,0.0,0.0])\n",
        "model_cloned = RPS_MLP_LSTM(input_dim, hidden_dim, output_dim)\n",
        "for episode in range(200):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "\n",
        "        # Generate a single sequence of RPS moves with skewed probabilities\n",
        "          # Probabilities for rock, paper, scissors\n",
        "        input_sequence = torch.multinomial(probabilities, sequence_length, replacement=True)\n",
        "        input_one_hot = F.one_hot(input_sequence,3)\n",
        "        #print(input_one_hot)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize the hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros(1, hidden_dim)\n",
        "        c0 = torch.zeros(1, hidden_dim)\n",
        "\n",
        "        # Pass the input sequence through the model\n",
        "\n",
        "        if(episode==0):\n",
        "                  #pass\n",
        "                  model_cloned.load_state_dict(model.state_dict())\n",
        "        opt_cloned = torch.optim.Adam(model_cloned.parameters(),1e-3)  ## Not used\n",
        "\n",
        "        outputs = model_cloned(input_one_hot.float())\n",
        "\n",
        "        m = Categorical(torch.softmax(outputs, dim=-1))\n",
        "\n",
        "\n",
        "            # Sample an action\n",
        "        action_ = m.sample()\n",
        "        log_prob = m.log_prob(action_)\n",
        "        print(action_)\n",
        "\n",
        "\n",
        "        # Compute the reward for each output\n",
        "        for i in range(sequence_length):\n",
        "            # Compute the action probabilities from the output\n",
        "\n",
        "            action = action_[i]\n",
        "\n",
        "            if(action ==input_sequence[i]):\n",
        "              reward = 0.0\n",
        "            elif(action==0 and input_sequence[i]==1):\n",
        "              reward = -1.0\n",
        "            elif(action==0 and input_sequence[i]==2):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==1 and input_sequence[i]==2):\n",
        "              reward = -1.0\n",
        "            elif(action==1 and input_sequence[i]==0):\n",
        "              reward = 1.0\n",
        "\n",
        "            elif(action==2 and input_sequence[i]==0):\n",
        "              reward = -1.0\n",
        "            elif(action==2 and input_sequence[i]==1):\n",
        "              reward = 1.0\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the reward based on the sampled action and actual move\n",
        "\n",
        "\n",
        "            # Store the log probability and reward\n",
        "            log_probs.append(log_prob[i])\n",
        "            rewards.append(reward)\n",
        "\n",
        "\n",
        "        # Compute the cumulative rewards\n",
        "        cum_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r + cumulative_reward\n",
        "            cum_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        # Compute the loss and update the model\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        cum_rewards = torch.tensor(cum_rewards)\n",
        "        loss = -torch.sum(log_probs * cum_rewards)\n",
        "        opt_cloned.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_cloned.step()\n",
        "        print(torch.sum(cum_rewards))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68tOGFDU2bIB",
        "outputId": "9d2ecbf5-8111-4ed8-cca3-58fe86687b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RPS_MLP_LSTM(\n",
            "  (lstm): LSTM(3, 32)\n",
            "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
            ")\n",
            "tensor([2, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 1, 2,\n",
            "        1, 2, 1, 1, 2, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 0, 2, 0, 2, 1, 2, 0,\n",
            "        2, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1, 2, 2, 2, 1,\n",
            "        1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 2, 1, 2, 0, 1, 2, 0, 1, 1, 1, 0,\n",
            "        1, 2, 0, 1])\n",
            "tensor(386.)\n",
            "tensor([1, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 0, 2,\n",
            "        0, 2, 2, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 2,\n",
            "        0, 2, 0, 1, 0, 2, 1, 0, 0, 2, 2, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 1,\n",
            "        1, 2, 2, 1, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 0, 2, 2, 1, 1, 0, 1, 2, 2, 0,\n",
            "        0, 2, 0, 2])\n",
            "tensor(-309.)\n",
            "tensor([1, 1, 2, 1, 0, 2, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 2,\n",
            "        1, 0, 1, 2, 2, 0, 1, 0, 0, 2, 0, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 0,\n",
            "        0, 2, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 2, 0, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0,\n",
            "        2, 1, 1, 1])\n",
            "tensor(842.)\n",
            "tensor([2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 1, 2, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2,\n",
            "        1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 2, 2, 1, 1, 2, 2, 2,\n",
            "        1, 1, 2, 2, 1, 1, 2, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 2, 2, 1, 1, 1, 0, 0,\n",
            "        2, 2, 2, 2, 2, 2, 1, 0, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 2, 2, 1])\n",
            "tensor(85.)\n",
            "tensor([1, 0, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 2, 0, 0, 2,\n",
            "        2, 2, 0, 0, 2, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 2, 2,\n",
            "        2, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2,\n",
            "        2, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1,\n",
            "        0, 0, 1, 1])\n",
            "tensor(1174.)\n",
            "tensor([0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1,\n",
            "        0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 1, 2, 1, 1,\n",
            "        1, 0, 2, 2, 2, 0, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 2,\n",
            "        2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 0, 2, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1, 1,\n",
            "        2, 1, 0, 2])\n",
            "tensor(114.)\n",
            "tensor([1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 2, 0, 0, 2, 0, 0, 1, 0, 2,\n",
            "        0, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2, 2,\n",
            "        2, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 2, 0, 2, 1, 1, 2, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1, 0, 2, 2, 2, 1, 1, 2, 1, 0, 1, 0, 2,\n",
            "        2, 1, 0, 0])\n",
            "tensor(435.)\n",
            "tensor([2, 1, 2, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2,\n",
            "        0, 1, 1, 1, 2, 0, 2, 0, 1, 0, 0, 2, 1, 2, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0,\n",
            "        2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 0, 2, 0, 2, 1, 2, 1, 0, 2, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 2, 1, 1, 2, 2, 1, 0, 1, 0, 2, 2, 1, 1, 0, 1, 2, 0, 1, 2,\n",
            "        2, 2, 1, 0])\n",
            "tensor(166.)\n",
            "tensor([2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 2, 2, 2, 1, 1,\n",
            "        2, 0, 1, 2, 2, 2, 2, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 0, 1,\n",
            "        0, 2, 2, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1,\n",
            "        2, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 2, 1, 2,\n",
            "        0, 0, 2, 2])\n",
            "tensor(206.)\n",
            "tensor([2, 1, 2, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1, 2, 0, 2, 2, 2, 1, 0, 0, 1, 1, 2,\n",
            "        0, 1, 2, 0, 2, 0, 2, 1, 1, 0, 2, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 2, 0, 1,\n",
            "        0, 1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 2, 2, 0,\n",
            "        0, 0, 2, 2, 2, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 0, 0, 2, 1, 2, 1, 2, 1,\n",
            "        0, 0, 0, 2])\n",
            "tensor(-216.)\n",
            "tensor([1, 0, 1, 2, 0, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 2,\n",
            "        0, 2, 2, 0, 1, 0, 2, 0, 0, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 1, 0, 2,\n",
            "        0, 2, 2, 0, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 2, 0, 2, 1,\n",
            "        1, 2, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 1, 2, 2, 2, 1, 1, 0, 2, 1, 0,\n",
            "        1, 2, 2, 2])\n",
            "tensor(-271.)\n",
            "tensor([0, 0, 0, 2, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 2, 2, 0, 1, 0, 0, 1, 0, 2, 1, 2, 2, 2, 1, 0, 1, 2, 0, 2,\n",
            "        1, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 1,\n",
            "        1, 1, 2, 0, 2, 1, 0, 1, 1, 0, 2, 1, 2, 2, 0, 1, 2, 1, 2, 1, 2, 2, 2, 2,\n",
            "        0, 0, 2, 1])\n",
            "tensor(46.)\n",
            "tensor([1, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 2, 1, 2, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 1,\n",
            "        2, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 0, 2,\n",
            "        1, 1, 0, 1])\n",
            "tensor(1146.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 2, 1,\n",
            "        2, 0, 1, 0, 1, 2, 2, 0, 2, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1,\n",
            "        1, 0, 1, 2, 0, 1, 2, 2, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 2, 1, 2, 0,\n",
            "        0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 2, 0, 1, 1,\n",
            "        0, 0, 1, 0])\n",
            "tensor(1113.)\n",
            "tensor([0, 0, 0, 2, 0, 2, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 0, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 0, 2, 1, 0, 1, 2,\n",
            "        2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 2, 2, 0, 0, 2, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0])\n",
            "tensor(1465.)\n",
            "tensor([2, 2, 0, 1, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2, 2, 1, 0, 1, 1, 1, 0, 2, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2,\n",
            "        2, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0,\n",
            "        2, 0, 1, 1, 1, 1, 2, 0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0,\n",
            "        2, 2, 1, 1])\n",
            "tensor(1151.)\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1,\n",
            "        2, 2, 1, 0, 0, 0, 2, 1, 1, 1, 2, 1, 0, 2, 0, 2, 1, 1, 2, 2, 2, 2, 1, 1,\n",
            "        0, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        0, 1, 2, 2, 1, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 2, 0,\n",
            "        0, 1, 2, 0])\n",
            "tensor(1038.)\n",
            "tensor([2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 2, 1, 2, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 2, 2, 0,\n",
            "        1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1, 1, 2, 2, 0,\n",
            "        1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 1, 1, 2,\n",
            "        0, 1, 1, 1])\n",
            "tensor(1416.)\n",
            "tensor([1, 1, 1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 0, 2,\n",
            "        0, 1, 1, 1, 0, 0, 0, 2, 2, 0, 2, 1, 2, 1, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2,\n",
            "        2, 1, 0, 1, 2, 2, 1, 1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1,\n",
            "        2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0,\n",
            "        1, 0, 1, 2])\n",
            "tensor(555.)\n",
            "tensor([1, 1, 0, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 0, 2, 2, 1, 1, 0, 0, 0, 1, 2, 2,\n",
            "        2, 1, 0, 1, 2, 1, 0, 2, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 2, 0, 2,\n",
            "        1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n",
            "        1, 1, 0, 0])\n",
            "tensor(1344.)\n",
            "tensor([1, 0, 0, 2, 0, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 2, 2,\n",
            "        1, 1, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 2, 0, 1, 2, 1, 2, 1,\n",
            "        1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 2, 2, 0, 2, 2,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1,\n",
            "        0, 1, 2, 2])\n",
            "tensor(641.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 1, 2, 1,\n",
            "        0, 2, 0, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 2, 2, 0, 1, 1, 1, 2, 0,\n",
            "        0, 0, 1, 2, 0, 2, 0, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 2, 2, 0, 2, 1, 2, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 0, 1,\n",
            "        0, 2, 2, 1])\n",
            "tensor(298.)\n",
            "tensor([2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
            "        2, 1, 1, 2, 2, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 0,\n",
            "        2, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 2, 1, 2, 1, 2, 0, 1,\n",
            "        2, 1, 1, 1, 2, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2,\n",
            "        0, 2, 0, 1])\n",
            "tensor(792.)\n",
            "tensor([1, 1, 0, 0, 2, 1, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 1, 1, 0,\n",
            "        1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2,\n",
            "        1, 2, 1, 0, 1, 2, 2, 2, 1, 2, 2, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0,\n",
            "        2, 1, 2, 1])\n",
            "tensor(402.)\n",
            "tensor([1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 1, 1, 2, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 2, 0, 0,\n",
            "        1, 0, 0, 1, 2, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        2, 1, 1, 2])\n",
            "tensor(1567.)\n",
            "tensor([1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 2, 0, 1, 2, 0, 2, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0,\n",
            "        2, 0, 1, 2, 1, 0, 1, 2, 2, 0, 1, 0, 1, 1, 0, 0, 1, 2, 2, 2, 0, 2, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(1315.)\n",
            "tensor([0, 0, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2,\n",
            "        1, 2, 1, 2, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1])\n",
            "tensor(2265.)\n",
            "tensor([0, 2, 0, 1, 1, 0, 0, 2, 1, 2, 2, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 2,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 2, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 1,\n",
            "        1, 0, 2, 2])\n",
            "tensor(1415.)\n",
            "tensor([2, 1, 2, 1, 2, 0, 1, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 2, 2, 2, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 2, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 2, 1, 1, 1,\n",
            "        1, 0, 1, 1, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 0, 0,\n",
            "        1, 1, 0, 0])\n",
            "tensor(1382.)\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 1, 1, 1, 0, 2,\n",
            "        1, 1, 0, 2, 1, 2, 1, 1, 2, 2, 0, 1, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 2, 1,\n",
            "        1, 2, 2, 0, 0, 1, 1, 1, 1, 1, 0, 2, 0, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 1,\n",
            "        2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 2, 0, 1, 1, 1, 1, 2, 1, 2, 2, 0,\n",
            "        1, 2, 1, 0])\n",
            "tensor(714.)\n",
            "tensor([0, 1, 1, 2, 2, 0, 0, 2, 2, 0, 1, 2, 0, 2, 2, 0, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "        0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 2,\n",
            "        0, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0,\n",
            "        1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 2, 2, 0,\n",
            "        0, 2, 1, 1])\n",
            "tensor(490.)\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 0, 1, 1, 2, 2, 0, 1, 2,\n",
            "        1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2,\n",
            "        0, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1,\n",
            "        0, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 2,\n",
            "        0, 2, 1, 1])\n",
            "tensor(1787.)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2,\n",
            "        2, 1, 1, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 2, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2,\n",
            "        0, 2, 1, 0, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2,\n",
            "        2, 2, 1, 1])\n",
            "tensor(1194.)\n",
            "tensor([2, 1, 0, 2, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 2, 0, 0, 1, 1, 1, 2, 0, 0, 1,\n",
            "        1, 2, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 2, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 0, 2, 1, 2,\n",
            "        1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 2, 0, 0, 0, 2, 1, 1, 2, 0, 2, 0,\n",
            "        1, 1, 2, 0])\n",
            "tensor(1103.)\n",
            "tensor([0, 2, 0, 1, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 0, 1, 1, 0, 0, 2, 1, 1, 2, 2,\n",
            "        0, 1, 2, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 2, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1,\n",
            "        2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 0, 1, 1, 2, 0, 1, 0, 2, 2,\n",
            "        1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2,\n",
            "        1, 2, 0, 2])\n",
            "tensor(-105.)\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 1, 2, 1, 0, 1, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        2, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0,\n",
            "        2, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 2, 0, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(1680.)\n",
            "tensor([2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 2, 1, 0,\n",
            "        0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 2,\n",
            "        2, 1, 1, 1, 2, 2, 1, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1, 2, 0, 0, 0,\n",
            "        2, 1, 2, 0])\n",
            "tensor(1376.)\n",
            "tensor([0, 1, 2, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 1, 2, 0, 0, 2, 1, 2, 1, 0, 2, 1,\n",
            "        0, 0, 1, 2, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 0, 2, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 0, 1, 0, 2, 0, 2, 0,\n",
            "        1, 0, 1, 1])\n",
            "tensor(2032.)\n",
            "tensor([1, 2, 2, 0, 1, 2, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1,\n",
            "        2, 1, 1, 2, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 2, 0, 2, 2, 1, 1, 1, 1, 2,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 2, 1, 1, 2, 1, 1, 0, 1, 2,\n",
            "        2, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(1514.)\n",
            "tensor([1, 2, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0, 0, 0,\n",
            "        1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 1,\n",
            "        1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2,\n",
            "        1, 0, 2, 2, 2, 0, 1, 2, 0, 2, 0, 0, 1, 1, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2,\n",
            "        1, 1, 2, 1])\n",
            "tensor(673.)\n",
            "tensor([0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2,\n",
            "        2, 1, 1, 0, 1, 0, 1, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1,\n",
            "        1, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 2, 0,\n",
            "        2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 1, 2, 1, 0,\n",
            "        1, 2, 0, 1])\n",
            "tensor(1306.)\n",
            "tensor([2, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 1, 2,\n",
            "        2, 1, 0, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 0, 1, 2, 2, 2, 1,\n",
            "        1, 1, 2, 1, 1, 2, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 2, 0,\n",
            "        1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 2, 1, 1])\n",
            "tensor(1543.)\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 0,\n",
            "        2, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1,\n",
            "        0, 0, 1, 0, 1, 2, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0,\n",
            "        2, 2, 1, 0])\n",
            "tensor(1804.)\n",
            "tensor([0, 1, 1, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0,\n",
            "        0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 0, 1, 1, 2, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 0, 1, 0, 0, 2, 2, 1, 2, 0,\n",
            "        2, 2, 1, 2])\n",
            "tensor(1011.)\n",
            "tensor([1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 2, 1, 2, 1, 2,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 2, 2, 0, 2, 1, 1, 2, 1,\n",
            "        1, 0, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 0, 2, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 0, 2, 1])\n",
            "tensor(1771.)\n",
            "tensor([2, 0, 1, 0, 1, 2, 0, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 0, 2, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 2, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2157.)\n",
            "tensor([1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2,\n",
            "        1, 0, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0,\n",
            "        1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 2, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1])\n",
            "tensor(2205.)\n",
            "tensor([0, 2, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1,\n",
            "        2, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 2, 1, 1, 0,\n",
            "        1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2539.)\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2,\n",
            "        1, 1, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 2, 0, 1, 2, 0, 1,\n",
            "        1, 2, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0, 1, 2, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(2007.)\n",
            "tensor([0, 1, 2, 2, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "        0, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1, 1, 2, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 2, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1,\n",
            "        1, 1, 0, 0])\n",
            "tensor(1806.)\n",
            "tensor([0, 2, 2, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 1, 0, 1, 1, 0,\n",
            "        0, 1, 1, 2, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 0, 2, 1, 1, 0, 1, 2, 2,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2,\n",
            "        1, 2, 1, 1])\n",
            "tensor(1805.)\n",
            "tensor([2, 1, 0, 1, 2, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        2, 2, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 2, 2,\n",
            "        0, 2, 2, 2])\n",
            "tensor(1686.)\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        2, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 2, 1, 2])\n",
            "tensor(2977.)\n",
            "tensor([0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1,\n",
            "        1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2927.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 1, 0, 1,\n",
            "        1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 2,\n",
            "        2, 0, 1, 1])\n",
            "tensor(2718.)\n",
            "tensor([2, 2, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1,\n",
            "        1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 1, 0,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2,\n",
            "        0, 1, 0, 1])\n",
            "tensor(2298.)\n",
            "tensor([1, 1, 0, 0, 1, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0,\n",
            "        1, 2, 1, 0])\n",
            "tensor(2577.)\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        1, 1, 0, 2])\n",
            "tensor(3491.)\n",
            "tensor([1, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(2944.)\n",
            "tensor([1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 0, 0, 0, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        2, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 0, 0, 1, 2, 2, 0, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        0, 1, 2, 1])\n",
            "tensor(3008.)\n",
            "tensor([2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 2, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 0, 2, 1, 1, 2, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2789.)\n",
            "tensor([2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 2,\n",
            "        0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1,\n",
            "        2, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0])\n",
            "tensor(2986.)\n",
            "tensor([0, 2, 2, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 0, 0, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(2467.)\n",
            "tensor([0, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        2, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3251.)\n",
            "tensor([1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1])\n",
            "tensor(3350.)\n",
            "tensor([1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        2, 2, 1, 1])\n",
            "tensor(3196.)\n",
            "tensor([2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 1, 2, 1,\n",
            "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 2])\n",
            "tensor(3092.)\n",
            "tensor([2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 1,\n",
            "        1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 2, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2498.)\n",
            "tensor([2, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 2, 2,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 2, 1])\n",
            "tensor(3093.)\n",
            "tensor([2, 2, 2, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1,\n",
            "        1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1,\n",
            "        0, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        2, 1, 0, 0])\n",
            "tensor(2704.)\n",
            "tensor([2, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 2, 1, 0, 2, 1, 1, 1, 0, 2,\n",
            "        1, 0, 0, 1])\n",
            "tensor(2638.)\n",
            "tensor([1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1,\n",
            "        0, 1, 0, 2, 2, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1,\n",
            "        0, 0, 0, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 2, 1])\n",
            "tensor(2610.)\n",
            "tensor([2, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        2, 1, 0, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1,\n",
            "        1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1])\n",
            "tensor(3131.)\n",
            "tensor([2, 2, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0])\n",
            "tensor(3308.)\n",
            "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1,\n",
            "        1, 1, 1, 2, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3885.)\n",
            "tensor([2, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        2, 2, 2, 1])\n",
            "tensor(3121.)\n",
            "tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3180.)\n",
            "tensor([1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 2,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3682.)\n",
            "tensor([2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 2, 1,\n",
            "        1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 0, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3264.)\n",
            "tensor([1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 2])\n",
            "tensor(3253.)\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0,\n",
            "        1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(3483.)\n",
            "tensor([0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        1, 2, 1, 1, 0, 1, 0, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 2])\n",
            "tensor(3286.)\n",
            "tensor([2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 2, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3211.)\n",
            "tensor([0, 2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(3374.)\n",
            "tensor([0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1,\n",
            "        1, 1, 0, 0])\n",
            "tensor(3237.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2,\n",
            "        1, 2, 1, 1])\n",
            "tensor(4084.)\n",
            "tensor([1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1,\n",
            "        1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0])\n",
            "tensor(3585.)\n",
            "tensor([1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(3749.)\n",
            "tensor([0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(3816.)\n",
            "tensor([2, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 2, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3372.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2, 1, 2, 1, 1, 1,\n",
            "        1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0,\n",
            "        0, 1, 0, 1])\n",
            "tensor(3220.)\n",
            "tensor([2, 0, 1, 2, 0, 1, 1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3747.)\n",
            "tensor([1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4202.)\n",
            "tensor([1, 2, 2, 2, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3509.)\n",
            "tensor([0, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 1, 1, 0])\n",
            "tensor(3610.)\n",
            "tensor([0, 1, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3350.)\n",
            "tensor([1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 2, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        2, 1, 1, 1])\n",
            "tensor(3638.)\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(4060.)\n",
            "tensor([0, 0, 1, 0, 2, 1, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4163.)\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(3619.)\n",
            "tensor([1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
            "        1, 2, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1,\n",
            "        0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2,\n",
            "        2, 0, 0, 1])\n",
            "tensor(2716.)\n",
            "tensor([2, 2, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3372.)\n",
            "tensor([2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 2,\n",
            "        0, 1, 1, 1])\n",
            "tensor(3266.)\n",
            "tensor([2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0,\n",
            "        1, 0, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3255.)\n",
            "tensor([1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0])\n",
            "tensor(2915.)\n",
            "tensor([2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 2, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3146.)\n",
            "tensor([1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 2])\n",
            "tensor(3464.)\n",
            "tensor([1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0,\n",
            "        2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0,\n",
            "        1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0,\n",
            "        2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3232.)\n",
            "tensor([2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2966.)\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0])\n",
            "tensor(3289.)\n",
            "tensor([2, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1])\n",
            "tensor(3424.)\n",
            "tensor([1, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 2, 2,\n",
            "        1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 0])\n",
            "tensor(3060.)\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1,\n",
            "        0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 2,\n",
            "        1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 2, 1, 0])\n",
            "tensor(2327.)\n",
            "tensor([2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 0, 1, 0, 0, 2, 0, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(3053.)\n",
            "tensor([2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1,\n",
            "        1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 0, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(2811.)\n",
            "tensor([0, 0, 1, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, 2, 2, 0, 1,\n",
            "        0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1,\n",
            "        0, 2, 1, 1])\n",
            "tensor(2533.)\n",
            "tensor([1, 2, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 1, 1, 2,\n",
            "        2, 0, 2, 1])\n",
            "tensor(2330.)\n",
            "tensor([0, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 2, 1, 2, 2, 0, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 0, 1, 1])\n",
            "tensor(3811.)\n",
            "tensor([1, 2, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3078.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3788.)\n",
            "tensor([0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 2, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 1,\n",
            "        1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 2, 1, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        0, 2, 1, 1])\n",
            "tensor(2554.)\n",
            "tensor([2, 1, 1, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1])\n",
            "tensor(3408.)\n",
            "tensor([0, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3413.)\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3814.)\n",
            "tensor([0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(3390.)\n",
            "tensor([0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(2833.)\n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2,\n",
            "        2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1])\n",
            "tensor(2779.)\n",
            "tensor([2, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 2, 0,\n",
            "        1, 0, 1, 1])\n",
            "tensor(3353.)\n",
            "tensor([2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2,\n",
            "        2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 0, 2, 1])\n",
            "tensor(3190.)\n",
            "tensor([2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3825.)\n",
            "tensor([1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 2, 1])\n",
            "tensor(3913.)\n",
            "tensor([1, 2, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 0])\n",
            "tensor(3921.)\n",
            "tensor([1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2,\n",
            "        2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 0, 1])\n",
            "tensor(3666.)\n",
            "tensor([2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3900.)\n",
            "tensor([1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 2])\n",
            "tensor(3630.)\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2,\n",
            "        1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3332.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4220.)\n",
            "tensor([0, 2, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0])\n",
            "tensor(3551.)\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3593.)\n",
            "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3957.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4267.)\n",
            "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1])\n",
            "tensor(3621.)\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3897.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3746.)\n",
            "tensor([1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4609.)\n",
            "tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4351.)\n",
            "tensor([0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 2, 1, 0, 1, 1,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3832.)\n",
            "tensor([1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 2])\n",
            "tensor(4416.)\n",
            "tensor([1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4480.)\n",
            "tensor([1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(3972.)\n",
            "tensor([0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(4854.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 2])\n",
            "tensor(3600.)\n",
            "tensor([1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4955.)\n",
            "tensor([2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4614.)\n",
            "tensor([2, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4534.)\n",
            "tensor([2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4598.)\n",
            "tensor([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4755.)\n",
            "tensor([1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4676.)\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4800.)\n",
            "tensor([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4810.)\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(4576.)\n",
            "tensor([1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1])\n",
            "tensor(4450.)\n",
            "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1])\n",
            "tensor(4149.)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4260.)\n",
            "tensor([1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 0, 1, 1])\n",
            "tensor(4619.)\n",
            "tensor([2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4758.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4177.)\n",
            "tensor([2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4585.)\n",
            "tensor([2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1])\n",
            "tensor(4427.)\n",
            "tensor([2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4625.)\n",
            "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4858.)\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4296.)\n",
            "tensor([0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4677.)\n",
            "tensor([2, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4310.)\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1,\n",
            "        1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1])\n",
            "tensor(4191.)\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4574.)\n",
            "tensor([2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4440.)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4577.)\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0,\n",
            "        1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 2, 1])\n",
            "tensor(3912.)\n",
            "tensor([1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4197.)\n",
            "tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4877.)\n",
            "tensor([2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4229.)\n",
            "tensor([2, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1])\n",
            "tensor(4694.)\n",
            "tensor([1, 1, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4692.)\n",
            "tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4815.)\n",
            "tensor([2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(4498.)\n",
            "tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4610.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4618.)\n",
            "tensor([2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4799.)\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1])\n",
            "tensor(4329.)\n",
            "tensor([0, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4469.)\n",
            "tensor([2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4758.)\n",
            "tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1])\n",
            "tensor(4790.)\n",
            "tensor([2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4844.)\n",
            "tensor([0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4871.)\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4763.)\n",
            "tensor([2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4839.)\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4738.)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "tensor(4926.)\n",
            "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0])\n",
            "tensor(4466.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}