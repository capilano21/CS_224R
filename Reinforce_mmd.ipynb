{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0g8dQY4gM8"
      },
      "outputs": [],
      "source": [
        "### Install Dependencies ###\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_kernel(x, y, sigma=1.0):\n",
        "    # Compute pairwise squared Euclidean distances\n",
        "    dist = torch.cdist(x, y, p=2, compute_mode=\"donot_use_mm_for_euclid_dist\")\n",
        "\n",
        "    # Compute Gaussian kernel matrix\n",
        "    kernel = torch.exp(-torch.pow(dist, 2) / (2 * sigma ** 2))\n",
        "\n",
        "    return kernel\n",
        "\n",
        "def mmd_loss(x, y, sigma=1.0):\n",
        "    n = x.size(0)\n",
        "    m = y.size(0)\n",
        "\n",
        "    # Compute kernel matrices\n",
        "    xx = gaussian_kernel(x, x, sigma)\n",
        "    yy = gaussian_kernel(y, y, sigma)\n",
        "    xy = gaussian_kernel(x, y, sigma)\n",
        "\n",
        "    # Compute MMD loss\n",
        "    loss = (torch.sum(xx) / (n * (n - 1))) + (torch.sum(yy) / (m * (m - 1))) - (2 * torch.sum(xy) / (n * m))\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "dgpKpVaJPPWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBF(nn.Module):\n",
        "\n",
        "    def __init__(self, n_kernels=5, mul_factor=2.0, bandwidth=None):\n",
        "        super().__init__()\n",
        "        self.bandwidth_multipliers = mul_factor ** (torch.arange(n_kernels) - n_kernels // 2)\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "    def get_bandwidth(self, L2_distances):\n",
        "        if self.bandwidth is None:\n",
        "            n_samples = L2_distances.shape[0]\n",
        "            return L2_distances.data.sum() / (n_samples ** 2 - n_samples)\n",
        "\n",
        "        return self.bandwidth\n",
        "\n",
        "    def forward(self, X):\n",
        "        L2_distances = torch.cdist(X, X) ** 2\n",
        "        return torch.exp(-L2_distances[None, ...] / (self.get_bandwidth(L2_distances) * self.bandwidth_multipliers)[:, None, None]).sum(dim=0)\n",
        "\n",
        "\n",
        "class MMDLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel=RBF()):\n",
        "        super().__init__()\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        K = self.kernel(torch.vstack([X, Y]))\n",
        "\n",
        "        X_size = X.shape[0]\n",
        "        XX = K[:X_size, :X_size].mean()\n",
        "        XY = K[:X_size, X_size:].mean()\n",
        "        YY = K[X_size:, X_size:].mean()\n",
        "        return XX - 2 * XY + YY"
      ],
      "metadata": {
        "id": "TTVVdXxhH1P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3W1L0SAMM-"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "WIN = 1.0\n",
        "LOSS = -1.0\n",
        "DRAW = 0.5\n",
        "EPS = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QKluSh4DFd"
      },
      "outputs": [],
      "source": [
        "class Board():\n",
        "  def __init__(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "    self.eps = EPS\n",
        "  def reset(self):\n",
        "    self.state =  torch.zeros(9)\n",
        "    self.marker = torch.tensor(1.0)\n",
        "    self.game_status = torch.tensor(0.0)\n",
        "    self.result = torch.tensor(0.0)\n",
        "  def play_move(self,pos):\n",
        "    if(self.state[pos]!=0.0):\n",
        "        raise Exception(\"You made an illegal move\")\n",
        "\n",
        "    self.state[pos]=self.marker\n",
        "    self.marker = self.marker *-1.0\n",
        "  def get_status(self):\n",
        "\n",
        "    winning_combinations = [[0,1,2],[3,4,5],[6,7,8],\n",
        "                            [0,3,6],[1,4,7],[2,5,8],\n",
        "                            [0,4,8],[2,4,6]]\n",
        "\n",
        "    for combination in winning_combinations:\n",
        "           if(self.state[combination[0]]==self.state[combination[1]]==self.state[combination[2]]!=0.0):\n",
        "               self.game_status = torch.tensor(1.0)\n",
        "               self.result = self.state[combination[0]]\n",
        "    if(torch.count_nonzero(self.state)==9.0):\n",
        "        self.game_status = torch.tensor(1.0)\n",
        "        self.result = torch.tensor(0.5)\n",
        "    return self.game_status,self.result\n",
        "  def get_mask(self):\n",
        "     mask = self.state==0.0\n",
        "     return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TicTacToeMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TicTacToePolicy:\n",
        "    def __init__(self):\n",
        "        self.model = TicTacToeMLP(9, 128, 9)  # Input: 9 (3x3 board), Hidden: 128, Output: 9 (actions)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-2)\n",
        "        self.eval = False\n",
        "        self.state = False\n",
        "\n",
        "    def get_action_probabilities(self, board):\n",
        "        if(self.state==True):\n",
        "          inp = board\n",
        "        else:\n",
        "           inp = board.state\n",
        "\n",
        "        logit = self.model(inp.clone())\n",
        "\n",
        "\n",
        "        # Mask out illegal actions\n",
        "        if(self.state==True):\n",
        "           mask = board==0\n",
        "        else:\n",
        "           mask =   board.get_mask() # Mask: 1 for legal actions, 0 for illegal actions\n",
        "\n",
        "        mask = mask.float()\n",
        "\n",
        "        logits = logit - 1e9 * (1 - mask)  # Apply large negative values to illegal actions\n",
        "        probabilities = F.softmax(logits, dim=0)\n",
        "        if(eval):\n",
        "          move = torch.argmax(probabilities)\n",
        "        m =   torch.distributions.categorical.Categorical(probs=probabilities)\n",
        "        move = m.sample()\n",
        "\n",
        "        step = torch.count_nonzero(mask)\n",
        "        if(self.state==False):\n",
        "           eps_temp = board.eps*(1-step/12)\n",
        "        else:\n",
        "           eps_temp=0.03\n",
        "\n",
        "        if(random.random()<eps_temp):\n",
        "          valid_moves = np.where(mask==1.0)[0]\n",
        "          move = np.random.choice(valid_moves)\n",
        "          move = torch.tensor(move)\n",
        "\n",
        "        log_prob = m.log_prob(move)\n",
        "        return log_prob,move.item(),probabilities"
      ],
      "metadata": {
        "id": "bCCbGXyRxl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oNIoTXAVEc8"
      },
      "outputs": [],
      "source": [
        "class RandomPlayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, board):\n",
        "        # Get a list of available actions\n",
        "        pos = np.where(board.get_mask()==1)[0]\n",
        "        move = np.random.choice(pos)\n",
        "\n",
        "\n",
        "        return move\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB3fhJfcUZsq"
      },
      "outputs": [],
      "source": [
        "board = Board()\n",
        "player_1 = TicTacToePolicy()\n",
        "player_2 = TicTacToePolicy()\n",
        "player_1.model.load_state_dict(torch.load(\"/content/drive/MyDrive/policy60000.pt\"))\n",
        "#player_2.model.load_state_dict(torch.load(\"/content/drive/MyDrive/policy.pt\"))\n",
        "\n",
        "player_3 = RandomPlayer()\n",
        "\n",
        "\n",
        "def games(strategy=\"target\"):\n",
        "  states = []\n",
        "  log_prob = []\n",
        "  if(strategy==\"target\"):\n",
        "    player = player_1\n",
        "  else:\n",
        "    player = player_2\n",
        "\n",
        "  board.reset()\n",
        "  while True:\n",
        "    states.append(board.state)\n",
        "    prob,move,actions = player.get_action_probabilities(board)\n",
        "    log_prob.append(prob)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "\n",
        "    move = player_3.get_action(board)\n",
        "\n",
        "    board.play_move(move)\n",
        "    status,reward =board.get_status()\n",
        "    if(status==1.0):\n",
        "       break\n",
        "  return states,reward,log_prob\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG78lMCAKiVG",
        "outputId": "50437296-bab3-4b5c-e75c-8b8ded028a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r runs"
      ],
      "metadata": {
        "id": "D45xPe0fKxhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Define Training Loop ##\n",
        "num_games = 1000000\n",
        "board.eps=0.3\n",
        "player_2.optimizer.zero_grad()\n",
        "vector = []\n",
        "reward_diff = []\n",
        "criterion = torch.nn.MSELoss()\n",
        "fn =MMDLoss()\n",
        "reward_a =0\n",
        "reward_b =0\n",
        "for playouts in range(num_games):\n",
        "\n",
        "  if(len(vector)>10000):\n",
        "    vector=[]\n",
        "\n",
        "  states,reward,log_prob= games()\n",
        "\n",
        "  states_1,reward_1,log_prob_1 = games(\"neural\")\n",
        "  log_prob_tensor = torch.stack(log_prob_1)\n",
        "  reward_scaled=  (reward_1-reward)**2\n",
        "  reward_tensor = torch.ones(log_prob_tensor.shape)*reward_scaled\n",
        "  grad_tensor = ((log_prob_tensor)*(reward_scaled)).sum()  ## Note the minus is not there because we are minimizing\n",
        "  grad_tensor.backward()\n",
        "  board.eps = board.eps-board.eps/(num_games-10000)\n",
        "\n",
        "  pick = np.random.choice(len(states_1))\n",
        "  if(torch.count_nonzero(states_1[pick])<9):\n",
        "            vector.append(states_1[pick])\n",
        "\n",
        "  if((playouts+1)%512==0):\n",
        "     indices = np.random.permutation(np.arange(len(vector)))\n",
        "     indices = indices[0:4]\n",
        "     actions_1 = []\n",
        "     actions_2 = []\n",
        "     vec = torch.stack(vector)\n",
        "     player_1.state= True\n",
        "     player_2.state= True\n",
        "\n",
        "     for entry in indices:\n",
        "         _,_,a1 = player_1.get_action_probabilities(vec[entry])\n",
        "\n",
        "         _,_,a2 = player_2.get_action_probabilities(vec[entry])\n",
        "         actions_1.append(a1)\n",
        "         actions_2.append(a2)\n",
        "     p1_actions = torch.stack(actions_1)\n",
        "     p2_actions = torch.stack(actions_2)\n",
        "     player_1.state= False\n",
        "     player_2.state= False\n",
        "\n",
        "\n",
        "     loss = fn(p1_actions.detach(),p2_actions)\n",
        "     loss = -loss\n",
        "     loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if((playouts+1)%512==0):\n",
        "\n",
        "\n",
        "     player_2.optimizer.step()\n",
        "     player_2.optimizer.zero_grad()\n",
        "     writer.add_scalar('Expected difference', reward, playouts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if(playouts%10000==0):\n",
        "     player_2.eval=True\n",
        "     loss = 0\n",
        "     draw = 0\n",
        "     win = 0\n",
        "     for i in range(1000):\n",
        "      _,reward,_ = games(\"neural\")\n",
        "      if(reward==1.0):\n",
        "        win+=1\n",
        "      elif(reward==-1.0):\n",
        "        loss+=1\n",
        "      else:\n",
        "        draw+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     writer.add_scalar(\"Win_percentage_random\", win*100,playouts)\n",
        "     writer.add_scalar(\"Loss_percentage_random\",loss*100,playouts)\n",
        "     writer.add_scalar(\"Draw_percentage_random\",draw*100,playouts)\n",
        "     print(\"Evaluation after\",playouts,\"games\")\n",
        "     print(\"Win\",win/1000)\n",
        "     print(\"Loss\",loss/1000)\n",
        "     print(\"Draw\",draw/1000)\n",
        "     player_1.eval=False\n",
        "### Final testing against minimax,minimx_random and random ###\n",
        "player_1.eval=True\n",
        "loss = 0\n",
        "draw = 0\n",
        "win = 0\n",
        "for i in range(1000):\n",
        "    _,reward,_ = games(player=\"random\")\n",
        "    if(reward==1.0):\n",
        "      win+=1\n",
        "    elif(reward==-1.0):\n",
        "      loss+=1\n",
        "    else:\n",
        "      draw+=1\n",
        "\n",
        "torch.save(player_2.model.state_dict(),\"/content/drive/MyDrive/MMD_60k.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pc2uHaTpief_",
        "outputId": "fca38683-91e8-4a1d-f49d-91f0c6040682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after 0 games\n",
            "Win 0.386\n",
            "Loss 0.278\n",
            "Draw 0.336\n",
            "Evaluation after 10000 games\n",
            "Win 0.474\n",
            "Loss 0.203\n",
            "Draw 0.323\n",
            "Evaluation after 20000 games\n",
            "Win 0.459\n",
            "Loss 0.211\n",
            "Draw 0.33\n",
            "Evaluation after 30000 games\n",
            "Win 0.435\n",
            "Loss 0.222\n",
            "Draw 0.343\n",
            "Evaluation after 40000 games\n",
            "Win 0.468\n",
            "Loss 0.209\n",
            "Draw 0.323\n",
            "Evaluation after 50000 games\n",
            "Win 0.464\n",
            "Loss 0.206\n",
            "Draw 0.33\n",
            "Evaluation after 60000 games\n",
            "Win 0.46\n",
            "Loss 0.212\n",
            "Draw 0.328\n",
            "Evaluation after 70000 games\n",
            "Win 0.454\n",
            "Loss 0.192\n",
            "Draw 0.354\n",
            "Evaluation after 80000 games\n",
            "Win 0.451\n",
            "Loss 0.193\n",
            "Draw 0.356\n",
            "Evaluation after 90000 games\n",
            "Win 0.465\n",
            "Loss 0.215\n",
            "Draw 0.32\n",
            "Evaluation after 100000 games\n",
            "Win 0.474\n",
            "Loss 0.207\n",
            "Draw 0.319\n",
            "Evaluation after 110000 games\n",
            "Win 0.448\n",
            "Loss 0.201\n",
            "Draw 0.351\n",
            "Evaluation after 120000 games\n",
            "Win 0.481\n",
            "Loss 0.187\n",
            "Draw 0.332\n",
            "Evaluation after 130000 games\n",
            "Win 0.48\n",
            "Loss 0.195\n",
            "Draw 0.325\n",
            "Evaluation after 140000 games\n",
            "Win 0.478\n",
            "Loss 0.197\n",
            "Draw 0.325\n",
            "Evaluation after 150000 games\n",
            "Win 0.468\n",
            "Loss 0.218\n",
            "Draw 0.314\n",
            "Evaluation after 160000 games\n",
            "Win 0.463\n",
            "Loss 0.218\n",
            "Draw 0.319\n",
            "Evaluation after 170000 games\n",
            "Win 0.47\n",
            "Loss 0.204\n",
            "Draw 0.326\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-54141ad36e2f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mstates_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neural\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mlog_prob_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mreward_scaled\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mreward_1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-35471a4ff69b>\u001b[0m in \u001b[0;36mgames\u001b[0;34m(strategy)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m        \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2d8c23e036b6>\u001b[0m in \u001b[0;36mget_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcombination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwinning_combinations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m            \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "board.reset()\n",
        "board.play_move(4)\n",
        "board.play_move(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "_,_,prob_1 = player_1.get_action_probabilities(board)\n",
        "_,_,prob_2 = player_2.get_action_probabilities(board)\n",
        "\n",
        "print(torch.argmax(prob_1),torch.argmax(prob_2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJnIM0qZv_5N",
        "outputId": "17d83827-21b5-44bd-fe38-8db2847d1c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8) tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(player_2.model.state_dict(),\"/content/drive/MyDrive/MMD_60K.pt\")"
      ],
      "metadata": {
        "id": "GjXoVJNb2Ln7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r runs_mmd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "yAktvrqBwQ5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}